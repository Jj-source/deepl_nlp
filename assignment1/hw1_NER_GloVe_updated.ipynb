{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CoNLL-2003 dataset task demonstrates the labeling of tokens for named entity recognition (NER), part-of-speech (POS) tagging, and chunking. Each component of the JSON object corresponds to a different layer of annotation for the sentence:\n",
    "\n",
    "1. **Tokens**: These are the individual words or punctuation marks from the text. In this case, the sentence \"EU rejects German call to boycott British lamb.\" is split into tokens:\n",
    "   - \"EU\"\n",
    "   - \"rejects\"\n",
    "   - \"German\"\n",
    "   - \"call\"\n",
    "   - \"to\"\n",
    "   - \"boycott\"\n",
    "   - \"British\"\n",
    "   - \"lamb\"\n",
    "   - \".\"\n",
    "\n",
    "2. **POS Tags**: This array contains the POS tags corresponding to each token. The tags are encoded as numbers, each representing a specific part of speech (like noun, verb, adjective). These numbers usually correspond to a tagging scheme such as the Penn Treebank POS tags:\n",
    "   - \"EU\" is tagged as 22, which represents a proper noun.\n",
    "   - \"rejects\" is tagged as 42, indicating a verb in present tense.\n",
    "   - And so forth.\n",
    "\n",
    "3. **Chunk Tags**: This array indicates phrase chunk boundaries and types (like NP for noun phrase, VP for verb phrase). Each number again corresponds to a specific type of phrase or boundary in a predefined scheme:\n",
    "   - \"EU\" is part of a noun phrase, hence 11.\n",
    "   - \"rejects\" begins a verb phrase, indicated by 21.\n",
    "   - The chunk tags help in parsing the sentence into linguistically meaningful phrases.\n",
    "\n",
    "4. **NER Tags**: These tags are used for named entity recognition. They identify whether each token is part of a named entity (like a person, location, organization) and the type of entity:\n",
    "   - \"EU\" is tagged as 3, denoting an organization.\n",
    "   - \"German\" and \"British\" are tagged as 7, indicating nationality or ethnicity.\n",
    "   - Other tokens are tagged as 0, meaning they are not recognized as part of any named entity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Homework: \n",
    "Load a NER dataset (e.g. CoNLL-2003) using the script provided below.\n",
    "   - Create a custom nn.Module class that takes Glove word embeddings as input, passes them through a linear layer, and outputs NER tags\n",
    "   - Train the model using cross-entropy loss and evaluate its performance using entity-level F1 score\n",
    "   - Analyze the model's predictions and visualize the confusion matrix to identify common errors\n",
    "2. Build a multi-layer perceptron (MLP) for NER using Glove embeddings\n",
    "   - Extend the previous exercise by creating an nn.Module class that defines an MLP architecture on top of Glove embeddings\n",
    "   - Experiment with different hidden layer sizes and number of layers\n",
    "   - Evaluate the trained model using entity-level precision, recall, and F1 scores\n",
    "   - Compare the performance of the MLP model with the simple linear model from exercise \n",
    "   - 1\n",
    "3. Explore the effects of different activation functions and regularization techniques for NER\n",
    "   - Modify the MLP model from exercise 2 to allow configurable activation functions (e.g. ReLU, tanh, sigmoid)\n",
    "   - Train models with different activation functions.)\n",
    "   - Visualize the learned entity embeddings using dimensionality reduction techniques like PCA or t-SNE (edited) \n",
    "   - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uv in /home/jj/github/deepl_nlp/.venv/lib/python3.12/site-packages (0.9.5)\n",
      "\u001b[2mUsing Python 3.12.3 environment at: /home/jj/github/deepl_nlp/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m9 packages\u001b[0m \u001b[2min 7ms\u001b[0m\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.3 environment at: /home/jj/github/deepl_nlp/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m9 packages\u001b[0m \u001b[2min 7ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install uv\n",
    "!uv pip install numpy pandas torch transformers datasets scikit-learn umap-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions \n",
    "1. download the **conll2003** from the following [link](\"https://data.deepai.org/conll2003.zip\")\n",
    "2. unzip the file\n",
    "3. download the glove embeddings from [link](\"https://huggingface.co/datasets/SLU-CSCI4750/glove.6B.100d.txt/resolve/main/glove.6B.100d.txt.gz\")\n",
    "4. unzip the glove embeddings file\n",
    "5. update the constants in the code below to point to the correct file paths on your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jj/github/deepl_nlp/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# basic python data science tooling\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datasets import (\n",
    "    Dataset, \n",
    "    DatasetDict, \n",
    "    Features, \n",
    "    Sequence, \n",
    "    ClassLabel, \n",
    "    Value,\n",
    ")\n",
    "\n",
    "# progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# deep learning stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "# constants for config\n",
    "LOCAL_DIR = \"/home/jj/github/deepl_nlp/assignment1/data/conll2003/\"\n",
    "GLOVE_EMBEDS_PATH = '/home/jj/github/deepl_nlp/assignment1/embeddings/glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_file = os.path.join(LOCAL_DIR, \"train.txt\")\n",
    "valid_file = os.path.join(LOCAL_DIR, \"valid.txt\")\n",
    "test_file = os.path.join(LOCAL_DIR, \"test.txt\")\n",
    "\n",
    "pos_names = [\n",
    "    '\"',\n",
    "    \"''\",\n",
    "    \"#\",\n",
    "    \"$\",\n",
    "    \"(\",\n",
    "    \")\",\n",
    "    \",\",\n",
    "    \".\",\n",
    "    \":\",\n",
    "    \"``\",\n",
    "    \"CC\",\n",
    "    \"CD\",\n",
    "    \"DT\",\n",
    "    \"EX\",\n",
    "    \"FW\",\n",
    "    \"IN\",\n",
    "    \"JJ\",\n",
    "    \"JJR\",\n",
    "    \"JJS\",\n",
    "    \"LS\",\n",
    "    \"MD\",\n",
    "    \"NN\",\n",
    "    \"NNP\",\n",
    "    \"NNPS\",\n",
    "    \"NNS\",\n",
    "    \"NN|SYM\",\n",
    "    \"PDT\",\n",
    "    \"POS\",\n",
    "    \"PRP\",\n",
    "    \"PRP$\",\n",
    "    \"RB\",\n",
    "    \"RBR\",\n",
    "    \"RBS\",\n",
    "    \"RP\",\n",
    "    \"SYM\",\n",
    "    \"TO\",\n",
    "    \"UH\",\n",
    "    \"VB\",\n",
    "    \"VBD\",\n",
    "    \"VBG\",\n",
    "    \"VBN\",\n",
    "    \"VBP\",\n",
    "    \"VBZ\",\n",
    "    \"WDT\",\n",
    "    \"WP\",\n",
    "    \"WP$\",\n",
    "    \"WRB\",\n",
    "]\n",
    "\n",
    "chunk_names = [\n",
    "    \"O\",\n",
    "    \"B-ADJP\",\n",
    "    \"I-ADJP\",\n",
    "    \"B-ADVP\",\n",
    "    \"I-ADVP\",\n",
    "    \"B-CONJP\",\n",
    "    \"I-CONJP\",\n",
    "    \"B-INTJ\",\n",
    "    \"I-INTJ\",\n",
    "    \"B-LST\",\n",
    "    \"I-LST\",\n",
    "    \"B-NP\",\n",
    "    \"I-NP\",\n",
    "    \"B-PP\",\n",
    "    \"I-PP\",\n",
    "    \"B-PRT\",\n",
    "    \"I-PRT\",\n",
    "    \"B-SBAR\",\n",
    "    \"I-SBAR\",\n",
    "    \"B-UCP\",\n",
    "    \"I-UCP\",\n",
    "    \"B-VP\",\n",
    "    \"I-VP\",\n",
    "]\n",
    "\n",
    "ner_names = [\n",
    "    \"O\",\n",
    "    \"B-PER\",\n",
    "    \"I-PER\",\n",
    "    \"B-ORG\",\n",
    "    \"I-ORG\",\n",
    "    \"B-LOC\",\n",
    "    \"I-LOC\",\n",
    "    \"B-MISC\",\n",
    "    \"I-MISC\",\n",
    "]\n",
    "\n",
    "\n",
    "def parse_conll(path: str):\n",
    "    \"\"\"Parse a CoNLL-2003 file into a list of examples.\"\"\"\n",
    "    examples = []\n",
    "    tokens, pos_tags, chunk_tags, ner_tags = [], [], [], []\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"-DOCSTART-\") or line.strip() == \"\":\n",
    "                if tokens:\n",
    "                    examples.append(\n",
    "                        {\n",
    "                            \"tokens\": tokens,\n",
    "                            \"pos_tags\": pos_tags,\n",
    "                            \"chunk_tags\": chunk_tags,\n",
    "                            \"ner_tags\": ner_tags,\n",
    "                        }\n",
    "                    )\n",
    "                    tokens, pos_tags, chunk_tags, ner_tags = [], [], [], []\n",
    "            else:\n",
    "                splits = line.rstrip().split(\" \")\n",
    "                tokens.append(splits[0])\n",
    "                pos_tags.append(splits[1])\n",
    "                chunk_tags.append(splits[2])\n",
    "                ner_tags.append(splits[3])\n",
    "    if tokens:\n",
    "        examples.append(\n",
    "            {\n",
    "                \"tokens\": tokens,\n",
    "                \"pos_tags\": pos_tags,\n",
    "                \"chunk_tags\": chunk_tags,\n",
    "                \"ner_tags\": ner_tags,\n",
    "            }\n",
    "        )\n",
    "    return examples\n",
    "\n",
    "\n",
    "def as_dataset(examples, features: Features):\n",
    "    ids = []\n",
    "    tokens_col, pos_col, chunk_col, ner_col = [], [], [], []\n",
    "    for i, ex in enumerate(examples):\n",
    "        ids.append(str(i))\n",
    "        tokens_col.append(ex[\"tokens\"])\n",
    "        pos_col.append(ex[\"pos_tags\"])\n",
    "        chunk_col.append(ex[\"chunk_tags\"])\n",
    "        ner_col.append(ex[\"ner_tags\"])\n",
    "    return Dataset.from_dict(\n",
    "        {\n",
    "            \"id\": ids,\n",
    "            \"tokens\": tokens_col,\n",
    "            \"pos_tags\": pos_col,\n",
    "            \"chunk_tags\": chunk_col,\n",
    "            \"ner_tags\": ner_col,\n",
    "        },\n",
    "        features=features,\n",
    "    )\n",
    "\n",
    "\n",
    "features = Features(\n",
    "    {\n",
    "        \"id\": Value(\"string\"),\n",
    "        \"tokens\": Sequence(Value(\"string\")),\n",
    "        \"pos_tags\": Sequence(ClassLabel(names=pos_names)),\n",
    "        \"chunk_tags\": Sequence(ClassLabel(names=chunk_names)),\n",
    "        \"ner_tags\": Sequence(ClassLabel(names=ner_names)),\n",
    "    }\n",
    ")\n",
    "\n",
    "train_examples = parse_conll(train_file)\n",
    "valid_examples = parse_conll(valid_file)\n",
    "test_examples = parse_conll(test_file)\n",
    "\n",
    "conll2003 = DatasetDict(\n",
    "    {\n",
    "        \"train\": as_dataset(train_examples, features),\n",
    "        \"validation\": as_dataset(valid_examples, features),\n",
    "        \"test\": as_dataset(test_examples, features),\n",
    "    }\n",
    ")\n",
    "\n",
    "display(conll2003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(conll2003['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(400000, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_glove_embeddings(file_path, embedding_dim):\n",
    "    # dict to store word embed vectors\n",
    "    word_vectors = {}\n",
    "    with open(file_path, 'r', encoding='utf - 8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = torch.tensor(\n",
    "                [float(val) for val in values[1:]], dtype=torch.float32)\n",
    "            word_vectors[word] = vector\n",
    "\n",
    "    # matrix of embeddings\n",
    "    vocab_size = len(word_vectors)\n",
    "    embedding_matrix = torch.zeros((vocab_size, embedding_dim))\n",
    "    word_to_idx = {}\n",
    "    idx_to_word = {}\n",
    "    for i, (word, vector) in enumerate(word_vectors.items()):\n",
    "        embedding_matrix[i] = vector\n",
    "        word_to_idx[word] = i\n",
    "        idx_to_word[i] = word\n",
    "\n",
    "    return embedding_matrix, word_to_idx, idx_to_word\n",
    "\n",
    "\n",
    "embedding_dim = 100\n",
    "embedding_matrix, word_to_idx, idx_to_word = load_glove_embeddings(GLOVE_EMBEDS_PATH, embedding_dim)\n",
    "\n",
    "\n",
    "embedding_layer = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index for unknown tokens (out-of-vocabulary words not in GloVe)\n",
    "UNK_IDX = len(word_to_idx)\n",
    "\n",
    "def tokens_to_indices(tokens_batch):\n",
    "    \"\"\"\n",
    "    Convert a batch of token sequences to their corresponding GloVe indices.\n",
    "    Args:\n",
    "        tokens_batch: List of token sequences (list of lists of strings)\n",
    "    Returns:\n",
    "        List of tensors containing token indices\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    for tokens in tokens_batch:\n",
    "        # Look up each token (lowercased) in word_to_idx, use UNK_IDX if not found\n",
    "        idxs = [\n",
    "            word_to_idx.get(t.lower(), UNK_IDX) for t in tokens\n",
    "        ]\n",
    "        indices.append(torch.tensor(idxs, dtype=torch.long))\n",
    "    return indices\n",
    "\n",
    "\n",
    "def labels_to_tensors(labels_batch):\n",
    "    \"\"\"\n",
    "    Convert a batch of NER label sequences to tensors.\n",
    "    Args:\n",
    "        labels_batch: List of label sequences (list of lists of ints)\n",
    "    Returns:\n",
    "        List of tensors containing label indices\n",
    "    \"\"\"\n",
    "    return [torch.tensor(lbls, dtype=torch.long) for lbls in labels_batch]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for DataLoader to batch and pad sequences.\n",
    "    Args:\n",
    "        batch: List of dataset samples, each with 'tokens' and 'ner_tags'\n",
    "    Returns:\n",
    "        Dictionary with padded 'input_ids' and 'labels' tensors\n",
    "    \"\"\"\n",
    "    # Convert tokens to indices\n",
    "    input_ids = tokens_to_indices([b[\"tokens\"] for b in batch])\n",
    "    \n",
    "    # Convert NER tags to tensors\n",
    "    labels = labels_to_tensors([b[\"ner_tags\"] for b in batch])\n",
    "    \n",
    "    # Pad sequences to the same length within the batch\n",
    "    # Padding value for input_ids is UNK_IDX (unknown token)\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=UNK_IDX)\n",
    "    \n",
    "    # Padding value for labels is -100 (ignored by CrossEntropyLoss)\n",
    "    labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    \n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "\n",
    "# Create DataLoaders for training, validation, and test sets\n",
    "# batch_size=32: process 32 sequences at a time\n",
    "# shuffle=True: randomly shuffle training data each epoch\n",
    "train_dataloader = DataLoader(conll2003[\"train\"], batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(conll2003[\"validation\"], batch_size=32, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(conll2003[\"test\"], batch_size=32, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Train Loss: 0.6490\n",
      "Epoch 2/3 - Train Loss: 0.2935\n",
      "Epoch 2/3 - Train Loss: 0.2935\n",
      "Epoch 3/3 - Train Loss: 0.2168\n",
      "Epoch 3/3 - Train Loss: 0.2168\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class LinearNER(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple linear model for Named Entity Recognition using GloVe embeddings.\n",
    "    Architecture: Embedding -> Linear -> Logits\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_matrix: torch.Tensor, num_tags: int):\n",
    "        super().__init__()\n",
    "        # if you use 100-dimensional GloVe vectors for a 50,000-word vocabulary,\n",
    "        # vocab_size would be 50,000 and embed_dim would be 100.\n",
    "        vocab_size, embed_dim = embedding_matrix.shape\n",
    "        \n",
    "        # Create embedding layer with extra slot for unknown tokens (UNK)\n",
    "        # nn.Embedding is just a big lookup table. It stores a vector for each word index.\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embed_dim)\n",
    "        \n",
    "        # Initialize embeddings with pre-trained GloVe vectors\n",
    "        # The last row (index vocab_size) is reserved for UNK tokens and initialized to zeros\n",
    "        # This tells PyTorch not to track gradients for the next steps.\n",
    "        # This is important because we are just initializing the weights, not training them yet.\n",
    "        with torch.no_grad():\n",
    "            # Copies the pre-trained embedding_matrix (your GloVe vectors) into the embedding layer's weight table.\n",
    "            self.embedding.weight[:vocab_size].copy_(embedding_matrix)\n",
    "            self.embedding.weight[vocab_size].zero_()\n",
    "        \n",
    "        # Linear classifier maps embedding dimension to number of NER tags\n",
    "        # This is the model's \"brain.\" It's a single linear (or \"fully-connected\") layer.\n",
    "        self.classifier = nn.Linear(embed_dim, num_tags)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids: (batch_size, sequence_length) - token indices\n",
    "        Returns:\n",
    "            logits: (batch_size, sequence_length, num_tags) - unnormalized scores for each tag\n",
    "        \"\"\"\n",
    "        # input_ids is a batch of sentences, where each word is an index (e.g., [[10, 45, 132], [7, 500, 9]]).\n",
    "        # This line looks up the embedding vector for every single index.\n",
    "        # If the input shape is (Batch_Size, Sequence_Length), the output emb shape is (Batch_Size, Sequence_Length, Embedding_Dimension).\n",
    "        emb = self.embedding(input_ids)           # (B, T, D) - embed each token\n",
    "        # This applies the same linear layer to each token's embedding in the sequence\n",
    "        # logits = raw scores like [1.2, -0.5, 3.1, 0.1]\n",
    "        # (Batch_Size, Sequence_Length, Num_Tags)\n",
    "        logits = self.classifier(emb)             # (B, T, C) - project to tag space\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Get number of NER tags from dataset (e.g., O, B-PER, I-PER, B-ORG, etc.)\n",
    "num_tags = len(conll2003[\"train\"].features[\"ner_tags\"].feature.names)\n",
    "\n",
    "# Initialize model with pre-trained GloVe embeddings\n",
    "model = LinearNER(embedding_matrix, num_tags)\n",
    "\n",
    "# CrossEntropyLoss with ignore_index=-100 to skip padding tokens in loss calculation\n",
    "# its also very comoda perchè prende i logit e le label\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Adam optimizer for parameter updates\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Move model to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    # An epoch is too large to process at once, so it's broken into smaller chunks\n",
    "    # called batches (handled by your train_dataloader)\n",
    "    for batch in train_dataloader:\n",
    "        # Move batch data to device (gpu)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        \n",
    "        # Zero gradients from previous step\n",
    "        # This line clears the gradients from the previous batch,\n",
    "        # ensuring we only update the model based on the current batch's error.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: get predictions\n",
    "        logits = model(input_ids)\n",
    "        \n",
    "        # Compute loss (flatten to 2D for CrossEntropyLoss: [batch*seq_len, num_tags])\n",
    "        # The model output logits is 3D: (B, T, C).\n",
    "        # The labels are 2D: (B, T).\n",
    "        #\n",
    "        # CrossEntropyLoss expects 2D logits (N, C) and 1D labels (N), where N is the total number of items.\n",
    "        #\n",
    "        # logits.view(-1, num_tags): This \"flattens\" the (B, T, C) tensor into (B*T, C). It essentially makes one giant list of all the tokens in the batch.\n",
    "        # labels.view(-1): This flattens the (B, T) tensor into (B*T).\n",
    "        #\n",
    "        # Now, the loss function compares the logits for every single token against its corresponding true label. Thanks to ignore_index=-100, any token where the label is -100 is skipped.\n",
    "        loss = criterion(logits.view(-1, num_tags), labels.view(-1))\n",
    "        \n",
    "        # Backward pass: compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss for monitoring\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Print average loss per batch for this epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {total_loss/len(train_dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework: \n",
    "Load a NER dataset (e.g. CoNLL-2003) using the script provided below.\n",
    "   - Create a custom nn.Module class that takes Glove word embeddings as input, passes them through a linear layer, and outputs NER tags\n",
    "   - Train the model using cross-entropy loss and evaluate its performance using entity-level F1 score\n",
    "   - Analyze the model's predictions and visualize the confusion matrix to identify common errors\n",
    "2. Build a multi-layer perceptron (MLP) for NER using Glove embeddings\n",
    "   - Extend the previous exercise by creating an nn.Module class that defines an MLP architecture on top of Glove embeddings\n",
    "   - Experiment with different hidden layer sizes and number of layers\n",
    "   - Evaluate the trained model using entity-level precision, recall, and F1 scores\n",
    "   - Compare the performance of the MLP model with the simple linear model from exercise \n",
    "   - 1\n",
    "3. Explore the effects of different activation functions and regularization techniques for NER\n",
    "   - Modify the MLP model from exercise 2 to allow configurable activation functions (e.g. ReLU, tanh, sigmoid)\n",
    "   - Train models with different activation functions.)\n",
    "   - Visualize the learned entity embeddings using dimensionality reduction techniques like PCA or t-SNE (edited) \n",
    "   - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Entity-level Evaluation on Validation Set\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 102/102 [00:00<00:00, 598.10it/s]\n",
      "Evaluating: 100%|██████████| 102/102 [00:00<00:00, 598.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5952\n",
      "Recall:    0.6866\n",
      "F1 Score:  0.6376\n",
      "\n",
      "True Positives:  4080\n",
      "False Positives: 2775\n",
      "False Negatives: 1862\n",
      "\n",
      "============================================================\n",
      "Entity-level Evaluation on Test Set\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 108/108 [00:00<00:00, 673.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.5196\n",
      "Recall:    0.6100\n",
      "F1 Score:  0.5612\n",
      "\n",
      "True Positives:  3445\n",
      "False Positives: 3185\n",
      "False Negatives: 2203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "why is this necessary?\n",
    "Token-level vs Entity-level evaluation are VERY different:\n",
    "\n",
    "Token-level (simpler but less meaningful):\n",
    "    Evaluates each token independently\n",
    "    If you predict [B-PER, O, O] instead of [B-PER, I-PER, O], you get 2/3 = 66% accuracy\n",
    "    Problem: This doesn't tell you if you correctly identified complete entities!\n",
    "    \n",
    "Entity-level (what NER really cares about):\n",
    "    An entity is only correct if both the entity type AND the complete span match exactly\n",
    "    If you predict \"John\" as B-PER but miss \"Smith\" (I-PER), you get:\n",
    "    Predicted: 1 incomplete entity\n",
    "    True: 1 complete entity\n",
    "    Result: 0% match because the spans don't match exactly\n",
    "\n",
    "\"\"\"\n",
    "def get_entities(tags, tag_names):\n",
    "    \"\"\"\n",
    "    Extract named entities from a sequence of NER tags using BIO tagging scheme.\n",
    "    BIO scheme: B-TYPE (begin), I-TYPE (inside), O (outside)\n",
    "    \n",
    "    Args:\n",
    "        tags: Sequence of tag IDs\n",
    "        tag_names: List mapping tag IDs to tag names (e.g., ['O', 'B-PER', 'I-PER', ...])\n",
    "    Returns:\n",
    "        Set of tuples: (entity_type, start_idx, end_idx) for each entity found\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    \n",
    "    for i, tag_id in enumerate(tags):\n",
    "        if tag_id == -100:\n",
    "            continue\n",
    "        tag = tag_names[tag_id]\n",
    "        \n",
    "        if tag.startswith('B-'):\n",
    "            # B- tag starts a new entity\n",
    "            if current_entity is not None:\n",
    "                entities.append(current_entity)\n",
    "            entity_type = tag[2:]\n",
    "            current_entity = (entity_type, i, i)\n",
    "        elif tag.startswith('I-'):\n",
    "            # I- tag continues the current entity\n",
    "            entity_type = tag[2:]\n",
    "            if current_entity is not None and current_entity[0] == entity_type:\n",
    "                # Extend current entity to include this token\n",
    "                current_entity = (current_entity[0], current_entity[1], i)\n",
    "            else:\n",
    "                # Mismatched I- tag (e.g., I-PER after B-ORG), treat as new entity\n",
    "                if current_entity is not None:\n",
    "                    entities.append(current_entity)\n",
    "                current_entity = (entity_type, i, i)\n",
    "        else:\n",
    "            # O tag ends the current entity\n",
    "            if current_entity is not None:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "    \n",
    "    # Save the last entity if sequence ended while inside an entity\n",
    "    if current_entity is not None:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    return set(entities)\n",
    "\n",
    "def compute_entity_f1(model, dataloader, tag_names, device):\n",
    "    \"\"\"\n",
    "    Compute entity-level precision, recall, and F1 score.\n",
    "    Entity-level: an entity is correct only if both type and span match exactly.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained NER model\n",
    "        dataloader: DataLoader with evaluation data\n",
    "        tag_names: List of NER tag names\n",
    "        device: Device to run evaluation on (CPU/GPU)\n",
    "    Returns:\n",
    "        Dictionary with precision, recall, F1, and counts\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_true_entities = []\n",
    "    all_pred_entities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Get model predictions\n",
    "            logits = model(input_ids)\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Process each sequence in the batch\n",
    "            for pred_seq, true_seq in zip(predictions.cpu().numpy(), labels.cpu().numpy()):\n",
    "                # Extract entities from predicted and true sequences\n",
    "                true_entities = get_entities(true_seq, tag_names)\n",
    "                pred_entities = get_entities(pred_seq, tag_names)\n",
    "                \n",
    "                all_true_entities.append(true_entities)\n",
    "                all_pred_entities.append(pred_entities)\n",
    "    \n",
    "    # Calculate entity-level metrics\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    for true_ents, pred_ents in zip(all_true_entities, all_pred_entities):\n",
    "        # Intersection: entities present in both true and predicted\n",
    "        true_positives += len(true_ents & pred_ents)\n",
    "        # Predicted but not in ground truth\n",
    "        false_positives += len(pred_ents - true_ents)\n",
    "        # In ground truth but not predicted\n",
    "        false_negatives += len(true_ents - pred_ents)\n",
    "    \n",
    "    # Compute precision, recall, and F1\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'true_positives': true_positives,\n",
    "        'false_positives': false_positives,\n",
    "        'false_negatives': false_negatives\n",
    "    }\n",
    "\n",
    "\n",
    "tag_names = conll2003[\"train\"].features[\"ner_tags\"].feature.names\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Entity-level Evaluation on Validation Set\")\n",
    "print(\"=\" * 60)\n",
    "val_metrics = compute_entity_f1(model, val_dataloader, tag_names, device)\n",
    "print(f\"Precision: {val_metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {val_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {val_metrics['f1']:.4f}\")\n",
    "print(f\"\\nTrue Positives:  {val_metrics['true_positives']}\")\n",
    "print(f\"False Positives: {val_metrics['false_positives']}\")\n",
    "print(f\"False Negatives: {val_metrics['false_negatives']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Entity-level Evaluation on Test Set\")\n",
    "print(\"=\" * 60)\n",
    "test_metrics = compute_entity_f1(model, test_dataloader, tag_names, device)\n",
    "print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"Recall:    {test_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {test_metrics['f1']:.4f}\")\n",
    "print(f\"\\nTrue Positives:  {test_metrics['true_positives']}\")\n",
    "print(f\"False Positives: {test_metrics['false_positives']}\")\n",
    "print(f\"False Negatives: {test_metrics['false_negatives']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in /home/jj/github/deepl_nlp/.venv/lib/python3.12/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/jj/github/deepl_nlp/.venv/lib/python3.12/site-packages (from seqeval) (2.3.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /home/jj/github/deepl_nlp/.venv/lib/python3.12/site-packages (from seqeval) (1.7.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/jj/github/deepl_nlp/.venv/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/jj/github/deepl_nlp/.venv/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/jj/github/deepl_nlp/.venv/lib/python3.12/site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting predictions for seqeval evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting seqeval predictions: 100%|██████████| 108/108 [00:00<00:00, 732.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Entity-level Classification Report (seqeval - Test Set)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC     0.6536    0.7908    0.7157      1668\n",
      "        MISC     0.4550    0.5470    0.4968       702\n",
      "         ORG     0.4907    0.5232    0.5064      1661\n",
      "         PER     0.4372    0.5399    0.4831      1617\n",
      "\n",
      "   micro avg     0.5196    0.6100    0.5612      5648\n",
      "   macro avg     0.5091    0.6002    0.5505      5648\n",
      "weighted avg     0.5190    0.6100    0.5603      5648\n",
      "\n",
      "\n",
      "Overall Entity-level F1-Score: 0.5612\n",
      "\n",
      "============================================================\n",
      "Entity-level Classification Report (seqeval - Validation Set)\n",
      "============================================================\n",
      "\n",
      "Overall Entity-level F1-Score: 0.5612\n",
      "\n",
      "============================================================\n",
      "Entity-level Classification Report (seqeval - Validation Set)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting seqeval predictions: 100%|██████████| 102/102 [00:00<00:00, 718.24it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC     0.7544    0.8176    0.7847      1837\n",
      "        MISC     0.5552    0.6161    0.5841       922\n",
      "         ORG     0.5141    0.6107    0.5583      1341\n",
      "         PER     0.5298    0.6466    0.5824      1842\n",
      "\n",
      "   micro avg     0.5952    0.6866    0.6376      5942\n",
      "   macro avg     0.5884    0.6728    0.6274      5942\n",
      "weighted avg     0.5996    0.6866    0.6398      5942\n",
      "\n",
      "\n",
      "Overall Entity-level F1-Score: 0.6376\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "def get_seqeval_predictions(model, dataloader, tag_names, device):\n",
    "    \"\"\"\n",
    "    Get predictions in format required by seqeval (list of lists of string labels).\n",
    "    \n",
    "    Args:\n",
    "        model: Trained NER model\n",
    "        dataloader: DataLoader with evaluation data\n",
    "        tag_names: List of NER tag names\n",
    "        device: Device to run evaluation on\n",
    "    Returns:\n",
    "        Tuple of (true_tags, pred_tags) where each is a list of sequences\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    true_tags = []\n",
    "    pred_tags = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Getting seqeval predictions\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            logits = model(input_ids)\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Process each sequence in the batch\n",
    "            for pred_seq, label_seq in zip(predictions.cpu().numpy(), labels.cpu().numpy()):\n",
    "                # Convert indices to tag names, filtering out padding tokens\n",
    "                true_seq = []\n",
    "                pred_seq_tags = []\n",
    "                \n",
    "                for pred, label in zip(pred_seq, label_seq):\n",
    "                    if label != -100:  # Skip padding tokens\n",
    "                        true_seq.append(tag_names[label])\n",
    "                        pred_seq_tags.append(tag_names[pred])\n",
    "                \n",
    "                if true_seq:  # Only add non-empty sequences\n",
    "                    true_tags.append(true_seq)\n",
    "                    pred_tags.append(pred_seq_tags)\n",
    "    \n",
    "    return true_tags, pred_tags\n",
    "\n",
    "\n",
    "# Get predictions in seqeval format\n",
    "print(\"Collecting predictions for seqeval evaluation...\")\n",
    "true_tags, pred_tags = get_seqeval_predictions(model, test_dataloader, tag_names, device)\n",
    "\n",
    "# Print entity-level classification report using seqeval\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Entity-level Classification Report (seqeval - Test Set)\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(true_tags, pred_tags, digits=4))\n",
    "\n",
    "# Overall entity-level F1 score\n",
    "f1 = f1_score(true_tags, pred_tags)\n",
    "print(f\"\\nOverall Entity-level F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Also evaluate on validation set\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Entity-level Classification Report (seqeval - Validation Set)\")\n",
    "print(\"=\" * 60)\n",
    "val_true_tags, val_pred_tags = get_seqeval_predictions(model, val_dataloader, tag_names, device)\n",
    "print(classification_report(val_true_tags, val_pred_tags, digits=4))\n",
    "val_f1 = f1_score(val_true_tags, val_pred_tags)\n",
    "print(f\"\\nOverall Entity-level F1-Score: {val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting predictions for confusion matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting predictions: 100%|██████████| 108/108 [00:00<00:00, 770.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Token-level Classification Report (Test Set)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "classification_report() got an unexpected keyword argument 'target_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mToken-level Classification Report (Test Set)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtag_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdigits\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Confusion matrix\u001b[39;00m\n\u001b[32m     37\u001b[39m cm = confusion_matrix(labels, preds)\n",
      "\u001b[31mTypeError\u001b[39m: classification_report() got an unexpected keyword argument 'target_names'"
     ]
    }
   ],
   "source": [
    "# Token-level confusion matrix and classification report\n",
    "def get_token_predictions(model, dataloader, device):\n",
    "    \"\"\"Get all predictions and labels at token level (excluding padding).\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Getting predictions\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            logits = model(input_ids)\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # Flatten and filter out padding tokens\n",
    "            for pred_seq, label_seq in zip(predictions.cpu().numpy(), labels.cpu().numpy()):\n",
    "                for pred, label in zip(pred_seq, label_seq):\n",
    "                    if label != -100:  # Ignore padding\n",
    "                        all_preds.append(pred)\n",
    "                        all_labels.append(label)\n",
    "    \n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "\n",
    "# Get predictions\n",
    "print(\"Collecting predictions for confusion matrix...\")\n",
    "preds, labels = get_token_predictions(model, test_dataloader, device)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Token-level Classification Report (Test Set)\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(labels, preds, target_names=tag_names, digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=tag_names, yticklabels=tag_names,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.title('Confusion Matrix - LinearNER Model (Test Set)', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze most common errors\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Top 10 Most Common Prediction Errors\")\n",
    "print(\"=\" * 60)\n",
    "errors = []\n",
    "for i in range(len(tag_names)):\n",
    "    for j in range(len(tag_names)):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            errors.append((tag_names[i], tag_names[j], cm[i, j]))\n",
    "\n",
    "errors.sort(key=lambda x: x[2], reverse=True)\n",
    "for true_tag, pred_tag, count in errors[:10]:\n",
    "    print(f\"True: {true_tag:10s} -> Predicted: {pred_tag:10s} | Count: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
