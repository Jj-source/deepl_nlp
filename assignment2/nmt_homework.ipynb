{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMT Homework (Self-Contained): EN→DE\n",
    "\n",
    "Train a translation model (English→German), measure perplexity and BLEU, save a checkpoint, and optionally export predictions for ML‑Arena.\n",
    "\n",
    "Focus: experiment with architectures (LSTM w/ attention, Transformer, decoding strategies) — not boilerplate. Core evaluation functions are provided to ensure consistent scoring across students.\n",
    "\n",
    "Data: the course staff provides `dataset_splits/` in the repo root. No additional setup is needed for data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "Use `install.sh` or `pip install -r requirements.txt` to set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66fa97dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#!pip install -r requirements.txt\n",
    "import torch, sys, os, math, random\n",
    "import torch.nn.functional as F\n",
    "print('PyTorch version:', torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "try: sys.stdout.reconfigure(line_buffering=True)\n",
    "except Exception: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b079d0",
   "metadata": {},
   "source": [
    "## 1. Shared Utilities (no external imports)\n",
    "Tokenization, vocabulary, dataset, collate, and fixed evaluation (PPL, NLL, BLEU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83763f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Iterable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "SPECIAL_TOKENS = {'pad': '<pad>', 'sos': '<sos>', 'eos': '<eos>', 'unk': '<unk>'}\n",
    "\n",
    "def simple_tokenize(s: str) -> List[str]:\n",
    "    return s.strip().lower().split()\n",
    "\n",
    "def read_split(path: str) -> List[Tuple[List[str], List[str]]]:\n",
    "    pairs: List[Tuple[List[str], List[str]]] = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.rstrip('\\n').split('\t')\n",
    "            if len(parts) < 2: continue\n",
    "            pairs.append((simple_tokenize(parts[0]), simple_tokenize(parts[1])))\n",
    "    return pairs\n",
    "\n",
    "def build_vocab(seqs: Iterable[List[str]], max_size: int | None = None) -> Dict[str, int]:\n",
    "    from collections import Counter\n",
    "    c = Counter();\n",
    "    for s in seqs: c.update(s)\n",
    "    itms = c.most_common(max_size) if max_size else c.items()\n",
    "    stoi = {SPECIAL_TOKENS['pad']:0, SPECIAL_TOKENS['sos']:1, SPECIAL_TOKENS['eos']:2, SPECIAL_TOKENS['unk']:3}\n",
    "    for w,_ in itms:\n",
    "        if w not in stoi: stoi[w] = len(stoi)\n",
    "    return stoi\n",
    "\n",
    "def encode(tokens: List[str], stoi: Dict[str,int], add_sos_eos: bool=False) -> List[int]:\n",
    "    ids = [stoi.get(t, stoi[SPECIAL_TOKENS['unk']]) for t in tokens]\n",
    "    if add_sos_eos: ids = [stoi[SPECIAL_TOKENS['sos']]] + ids + [stoi[SPECIAL_TOKENS['eos']] ]\n",
    "    return ids\n",
    "\n",
    "class Example:\n",
    "    def __init__(self, s: List[int], ti: List[int], to: List[int]): self.src_ids=s; self.tgt_in_ids=ti; self.tgt_out_ids=to\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, pairs, src_stoi, tgt_stoi):\n",
    "        self.examples: List[Example] = []\n",
    "        for src, tgt in pairs:\n",
    "            s = encode(src, src_stoi) + [src_stoi[SPECIAL_TOKENS['eos']]]\n",
    "            t = encode(tgt, tgt_stoi, add_sos_eos=True)\n",
    "            self.examples.append(Example(s, t[:-1], t[1:]))\n",
    "    def __len__(self): return len(self.examples)\n",
    "    def __getitem__(self, i): return self.examples[i]\n",
    "\n",
    "def collate_pad(batch, pad_id_src: int, pad_id_tgt: int):\n",
    "    src_max = max(len(x.src_ids) for x in batch); tgt_max = max(len(x.tgt_in_ids) for x in batch)\n",
    "    def pad_to(a, L, pad): return a + [pad]*(L-len(a))\n",
    "    src    = torch.tensor([pad_to(x.src_ids,    src_max, pad_id_src) for x in batch])\n",
    "    tgt_in = torch.tensor([pad_to(x.tgt_in_ids, tgt_max, pad_id_tgt) for x in batch])\n",
    "    tgt_out= torch.tensor([pad_to(x.tgt_out_ids,tgt_max, pad_id_tgt) for x in batch])\n",
    "    src_l  = torch.tensor([len(x.src_ids)    for x in batch])\n",
    "    tgt_l  = torch.tensor([len(x.tgt_out_ids)for x in batch])\n",
    "    return src, src_l, tgt_in, tgt_out, tgt_l\n",
    "\n",
    "def compute_perplexity(loss_sum: float, token_count: int) -> float:\n",
    "    if token_count==0: return float('inf')\n",
    "    try: return float(math.exp(loss_sum/token_count))\n",
    "    except OverflowError: return float('inf')\n",
    "\n",
    "def corpus_bleu(refs: List[List[str]], hyps: List[List[str]], max_order: int=4, smooth: bool=True) -> float:\n",
    "    from collections import Counter\n",
    "    def ngrams(t,n): return Counter([tuple(t[i:i+n]) for i in range(len(t)-n+1)])\n",
    "    m=[0]*max_order; p=[0]*max_order; rl=0; hl=0\n",
    "    for r,h in zip(refs,hyps):\n",
    "        rl+=len(r); hl+=len(h);\n",
    "        for n in range(1,max_order+1):\n",
    "            R=ngrams(r,n); H=ngrams(h,n);\n",
    "            m[n-1]+=sum(min(c,H[g]) for g,c in R.items()); p[n-1]+=max(len(h)-n+1,0)\n",
    "    prec=[(m[i]+1)/(p[i]+1) if smooth else (m[i]/p[i] if p[i]>0 else 0.0) for i in range(max_order)]\n",
    "    geo=math.exp(sum((1/max_order)*math.log(x) for x in prec if x>0)) if min(prec)>0 else 0.0\n",
    "    bp=1.0 if hl>rl else math.exp(1-rl/max(1,hl))\n",
    "    return float(geo*bp)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_nll(loader: DataLoader, model: nn.Module, pad_id_tgt: int, device: torch.device):\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_id_tgt, reduction='sum')\n",
    "    model.eval(); tot=0.0; toks=0\n",
    "    for src,src_l,tgt_in,tgt_out,tgt_l in loader:\n",
    "        src,src_l = src.to(device), src_l.to(device)\n",
    "        tgt_in,tgt_out = tgt_in.to(device), tgt_out.to(device)\n",
    "        logits = model(src, src_l, tgt_in)\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "        tot+=float(loss.item()); toks+=int((tgt_out!=pad_id_tgt).sum().item())\n",
    "    return tot, toks\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_bleu(loader: DataLoader, model: nn.Module, tgt_itos: List[str], sos_id: int, eos_id: int, device: torch.device, max_len: int=100):\n",
    "    model.eval(); refs=[]; hyps=[]\n",
    "    for src,src_l,tgt_in,tgt_out,tgt_l in loader:\n",
    "        src,src_l = src.to(device), src_l.to(device)\n",
    "        pred = model.greedy_decode(src, src_l, max_len=max_len, sos_id=sos_id, eos_id=eos_id)\n",
    "        for b in range(src.size(0)):\n",
    "            ref_ids = tgt_out[b].tolist(); hyp_ids = pred[b].tolist()\n",
    "            if eos_id in ref_ids: ref_ids = ref_ids[:ref_ids.index(eos_id)]\n",
    "            if eos_id in hyp_ids: hyp_ids = hyp_ids[:hyp_ids.index(eos_id)]\n",
    "            refs.append([tgt_itos[i] for i in ref_ids if i!=0])\n",
    "            hyps.append([tgt_itos[i] for i in hyp_ids if i!=0 and i!=sos_id])\n",
    "    return float(corpus_bleu(refs, hyps))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f94ec5",
   "metadata": {},
   "source": [
    "## 2. Paths and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7db98fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public test path: dataset_splits/public_test.txt\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "train_path = 'dataset_splits/train.txt'\n",
    "val_path   = 'dataset_splits/val.txt'\n",
    "public_test_path = 'dataset_splits/public_test.txt'\n",
    "if not os.path.exists(public_test_path):\n",
    "    alt = 'dataset_splits/test_public.txt'\n",
    "    public_test_path = alt if os.path.exists(alt) else public_test_path\n",
    "private_test_path = 'dataset_splits/private_test.txt'\n",
    "src_vocab_size = 30000; tgt_vocab_size = 30000\n",
    "emb_dim = 256; hid_dim = 512; layers = 1; dropout = 0.1\n",
    "batch_size = 64; epochs = 5; lr = 3e-4; max_decode_len = 100\n",
    "save_dir = 'checkpoints'; os.makedirs(save_dir, exist_ok=True)\n",
    "print('Public test path:', public_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9fe355",
   "metadata": {},
   "source": [
    "## 3. Load Data and Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c98b8d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading splits...\n",
      "Train: 226,997 | Val: 32,428 | Public test: 32,428\n",
      "Vocab sizes — src: 30004 tgt: 30004\n"
     ]
    }
   ],
   "source": [
    "print('Loading splits...')\n",
    "train_pairs = read_split(train_path); val_pairs = read_split(val_path); test_pairs = read_split(public_test_path)\n",
    "print(f'Train: {len(train_pairs):,} | Val: {len(val_pairs):,} | Public test: {len(test_pairs):,}')\n",
    "src_stoi = build_vocab((s for s,_ in train_pairs), max_size=src_vocab_size)\n",
    "tgt_stoi = build_vocab((t for _,t in train_pairs), max_size=tgt_vocab_size)\n",
    "pad_id_src = src_stoi[SPECIAL_TOKENS['pad']]; pad_id_tgt = tgt_stoi[SPECIAL_TOKENS['pad']]\n",
    "sos_id = tgt_stoi[SPECIAL_TOKENS['sos']]; eos_id = tgt_stoi[SPECIAL_TOKENS['eos']]\n",
    "train_ds = TranslationDataset(train_pairs, src_stoi, tgt_stoi); val_ds = TranslationDataset(val_pairs, src_stoi, tgt_stoi); test_ds = TranslationDataset(test_pairs, src_stoi, tgt_stoi)\n",
    "collate = lambda b: collate_pad(b, pad_id_src, pad_id_tgt)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  collate_fn=collate, num_workers=0)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, collate_fn=collate, num_workers=0)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, collate_fn=collate, num_workers=0)\n",
    "tgt_itos = [None]*len(tgt_stoi);\n",
    "for w,i in tgt_stoi.items():\n",
    "    if 0<=i<len(tgt_itos): tgt_itos[i]=w\n",
    "print('Vocab sizes — src:', len(src_stoi), 'tgt:', len(tgt_stoi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb90b487",
   "metadata": {},
   "source": [
    "## 4. Build Model (Your Playground)\n",
    "Keep the forward/greedy_decode contract so evaluation works. Try adding attention, GRU, Transformer, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cebcb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33908020"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "  \n",
    "  def __init__(self, vocab_size, emb_dim, hid_dim, num_layers=1, dropout=0.1):\n",
    "    \n",
    "    super().__init__()\n",
    "    # converts token IDs to dense vectors, padding_idx=0 means <pad> gets zeroed out and doesn't learn\n",
    "    self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "    # LSTM takes embeddings and produces hidden states, batch_first means input is [batch, seq, features]\n",
    "    # dropout between LSTM layers only makes sense if num_layers > 1, otherwise skip it\n",
    "    self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, batch_first=True, \n",
    "                       dropout=dropout if num_layers > 1 else 0.0)\n",
    "    \n",
    "  def forward(self, src, src_lens):\n",
    "    \n",
    "    # turn token IDs into vectors: [B, S] -> [B, S, emb_dim]\n",
    "    emb = self.emb(src)\n",
    "    # pack removes padding so LSTM doesn't waste computation on <pad> tokens\n",
    "    # enforce_sorted=False handles sequences in any order (slightly slower but easier)\n",
    "    packed = nn.utils.rnn.pack_padded_sequence(emb, src_lens.cpu(), batch_first=True, enforce_sorted=False)\n",
    "    # LSTM returns: outputs for all timesteps + final hidden state (h, c)\n",
    "    out, (h, c) = self.rnn(packed)\n",
    "    # unpack puts padding back so we get regular [B, S, hid_dim] tensor\n",
    "    out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "    # return both: encoder outputs and final (h, c) which initializes decoder\n",
    "    # what are encoder outputs? they are the hidden states for each time step in the input sequence \n",
    "    return out, (h, c)\n",
    "  \n",
    "class Decoder(nn.Module):\n",
    "  \n",
    "  def __init__(self, vocab_size, emb_dim, hid_dim, num_layers=1, dropout=0.1):\n",
    "      \n",
    "    super().__init__()\n",
    "    self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "    # decoder LSTM takes previous hidden state and current token embedding\n",
    "    self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, batch_first=True, \n",
    "                       dropout=dropout if num_layers > 1 else 0.0)\n",
    "    # linear layer converts hidden state to vocab logits for prediction\n",
    "    self.proj = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "  def forward(self, tgt_in, hidden):\n",
    "      \n",
    "    # convert target token IDs to embeddings: [B, T] -> [B, T, emb_dim]\n",
    "    emb = self.emb(tgt_in)\n",
    "    # LSTM uses previous hidden state (from encoder or prev step) to process current tokens\n",
    "    out, hidden = self.rnn(emb, hidden)\n",
    "    # project LSTM output to vocab size: [B, T, hid_dim] -> [B, T, vocab_size]\n",
    "    # return new hidden state so we can keep generating next tokens\n",
    "    return self.proj(out), hidden\n",
    "        \n",
    "class Seq2Seq(nn.Module):\n",
    "  \n",
    "    def __init__(self, enc, dec):\n",
    "      super().__init__()\n",
    "      self.encoder = enc\n",
    "      self.decoder = dec\n",
    "      \n",
    "    def forward(self, src, src_lens, tgt_in):\n",
    "      # encode source sentence, throw away encoder outputs, keep final hidden state\n",
    "      _, h = self.encoder(src, src_lens)\n",
    "      # decode using encoder's final state as initial state, tgt_in has all target tokens (teacher forcing)\n",
    "      logits, _ = self.decoder(tgt_in, h)\n",
    "      # return logits for cross-entropy loss\n",
    "      return logits\n",
    "      \n",
    "    @torch.no_grad()\n",
    "    def greedy_decode(self, src, src_lens, max_len, sos_id, eos_id):\n",
    "      \n",
    "        B = src.size(0)\n",
    "        # encode source, get initial hidden state for decoder\n",
    "        _, h = self.encoder(src, src_lens)\n",
    "        # create batch of <sos> tokens to start generation: [B, 1]\n",
    "        inputs = torch.full((B, 1), sos_id, dtype=torch.long, device=src.device)\n",
    "        outs = []\n",
    "        # generate one token at a time up to max_len\n",
    "        for _ in range(max_len):\n",
    "            # feed only last generated token with current hidden state\n",
    "            logits, h = self.decoder(inputs, h)\n",
    "            # pick token with highest probability (greedy): [B, 1]\n",
    "            nxt = logits[:, -1, :].argmax(-1, keepdim=True)\n",
    "            outs.append(nxt)\n",
    "            inputs = nxt\n",
    "            \n",
    "        # concatenate all generated tokens: [B, max_len]\n",
    "        seqs = torch.cat(outs, dim=1)\n",
    "        # truncate each sequence at first <eos> and fill rest with <eos>\n",
    "        for i in range(B):\n",
    "            row = seqs[i]\n",
    "            # if sequence generated <eos>, stop there\n",
    "            if (row == eos_id).any():\n",
    "              # find index of first <eos>\n",
    "              idx = (row == eos_id).nonzero(as_tuple=False)[0].item()\n",
    "              # fill everything after with <eos>\n",
    "              row[idx+1:] = eos_id\n",
    "        return seqs\n",
    "      \n",
    "# build encoder with source vocabulary size\n",
    "encoder = Encoder(len(src_stoi), emb_dim, hid_dim, num_layers=layers, dropout=dropout)\n",
    "# build decoder with target vocabulary size (German vocab != English vocab)\n",
    "decoder = Decoder(len(tgt_stoi), emb_dim, hid_dim, num_layers=layers, dropout=dropout)\n",
    "# wrap both in seq2seq model and move to GPU/CPU\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "# Adam optimizer for gradient descent\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# count total trainable parameters\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e8ee83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attentions\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "\n",
    "class LuongAttention(nn.Module):\n",
    "  def __init__(self, query_dim, key_dim, score_type='general'):\n",
    "    super().__init__()\n",
    "    self.score_type = score_type\n",
    "    if score_type == 'general':\n",
    "        self.Wa = nn.Linear(query_dim, key_dim, bias=False)  # Project query to key space\n",
    "\n",
    "  def forward(self, query, keys):\n",
    "      if self.score_type == 'dot' and query.size(-1) == keys.size(-1):\n",
    "          scores = torch.bmm(query, keys.transpose(1, 2))\n",
    "      elif self.score_type == 'general':\n",
    "          scores = torch.bmm(self.Wa(query), keys.transpose(1, 2))  # Now correct\n",
    "      \n",
    "      weights = F.softmax(scores, dim=-1)\n",
    "      context = torch.bmm(weights, keys)  # [B, 1, key_dim]\n",
    "      \n",
    "      return context, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30212ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LuongAttention.__init__() missing 1 required positional argument: 'key_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 105\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# build with attention\u001b[39;00m\n\u001b[32m    104\u001b[39m encoder = Encoder(\u001b[38;5;28mlen\u001b[39m(src_stoi), emb_dim, hid_dim, num_layers=layers, dropout=dropout)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m decoder = \u001b[43mDecoderAttn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtgt_stoi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhid_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    108\u001b[39m model = Seq2SeqAttn(encoder, decoder).to(device)\n\u001b[32m    109\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mDecoderAttn.__init__\u001b[39m\u001b[34m(self, vocab_size, emb_dim, hid_dim, num_layers, dropout, attn)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m     14\u001b[39m \u001b[38;5;28mself\u001b[39m.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28mself\u001b[39m.attention = \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhid_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.rnn = nn.LSTM(\n\u001b[32m     18\u001b[39m         emb_dim + hid_dim,\n\u001b[32m     19\u001b[39m         hid_dim,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m         dropout=dropout \u001b[38;5;28;01mif\u001b[39;00m num_layers > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.0\u001b[39m,\n\u001b[32m     23\u001b[39m     )\n\u001b[32m     25\u001b[39m \u001b[38;5;28mself\u001b[39m.proj = nn.Linear(hid_dim, vocab_size)\n",
      "\u001b[31mTypeError\u001b[39m: LuongAttention.__init__() missing 1 required positional argument: 'key_dim'"
     ]
    }
   ],
   "source": [
    "class DecoderAttn(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        emb_dim,\n",
    "        hid_dim,\n",
    "        num_layers=1,\n",
    "        dropout=0.1,\n",
    "        attn=LuongAttention\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "\n",
    "        self.attention = attn(hid_dim, hid_dim, score_type='dot')\n",
    "        self.rnn = nn.LSTM(\n",
    "                emb_dim + hid_dim,\n",
    "                hid_dim,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                dropout=dropout if num_layers > 1 else 0.0,\n",
    "            )\n",
    "\n",
    "        self.proj = nn.Linear(hid_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward_step(self, tgt_in, hidden, encoder_outputs):\n",
    "      \"\"\"Single timestep decode with attention.\"\"\"\n",
    "      emb = self.dropout(self.emb(tgt_in))  # [B, 1, emb_dim]\n",
    "      h = hidden[0][-1:].permute(1, 0, 2)  # [B, 1, hid_dim]\n",
    "      \n",
    "      # the attention thing, calculated on the hidden states of the encoder\n",
    "      context, attn_weights = self.attention(h, encoder_outputs)  # [B, 1, hid_dim]\n",
    "      emb = torch.cat([emb, context], dim=2)  # [B, 1, emb_dim + hid_dim]\n",
    "      out, hidden = self.rnn(emb, hidden)  # [B, 1, hid_dim]\n",
    "      logits = self.proj(out)  # [B, 1, vocab_size]\n",
    "      return logits, hidden, attn_weights\n",
    "\n",
    "    def forward(self, tgt_in, hidden, encoder_outputs):\n",
    "      \"\"\"Process entire target sequence with per-token attention (teacher forcing).\"\"\"\n",
    "      batch_size, seq_len = tgt_in.size()\n",
    "      outputs = []\n",
    "      attn_weights = []\n",
    "      \n",
    "      for i in range(seq_len):\n",
    "          token = tgt_in[:, i:i+1]  # [B, 1]\n",
    "          logits, hidden, attn_weights_step = self.forward_step(token, hidden, encoder_outputs)\n",
    "          outputs.append(logits)\n",
    "          attn_weights.append(attn_weights_step)\n",
    "      \n",
    "      outputs = torch.cat(outputs, dim=1)  # [B, T, vocab_size]\n",
    "      attn_weights = torch.cat(attn_weights, dim=1)  # [B, T, S]\n",
    "      return outputs, hidden, attn_weights\n",
    "\n",
    "\n",
    "class Seq2SeqAttn(nn.Module):\n",
    "\n",
    "    def __init__(self, enc, dec):\n",
    "        super().__init__()\n",
    "        self.encoder = enc\n",
    "        self.decoder = dec\n",
    "\n",
    "    def forward(self, src, src_lens, tgt_in):\n",
    "        enc_out, h = self.encoder(src, src_lens)\n",
    "\n",
    "        logits, _, _ = self.decoder(tgt_in, h, enc_out)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def greedy_decode(self, src, src_lens, max_len, sos_id, eos_id):\n",
    "\n",
    "        B = src.size(0)\n",
    "        # encode source, get initial hidden state for decoder\n",
    "        enc_out, h = self.encoder(src, src_lens)\n",
    "        # create batch of <sos> tokens to start generation: [B, 1]\n",
    "        inputs = torch.full((B, 1), sos_id, dtype=torch.long, device=src.device)\n",
    "        outs = []\n",
    "        # generate one token at a time up to max_len\n",
    "        for _ in range(max_len):\n",
    "            # feed only last generated token with current hidden state\n",
    "            logits, h, _ = self.decoder(inputs, h, enc_out)\n",
    "            # pick token with highest probability (greedy): [B, 1]\n",
    "            nxt = logits[:, -1, :].argmax(-1, keepdim=True)\n",
    "            outs.append(nxt)\n",
    "            inputs = nxt\n",
    "\n",
    "        # concatenate all generated tokens: [B, max_len]\n",
    "        seqs = torch.cat(outs, dim=1)\n",
    "        # truncate each sequence at first <eos> and fill rest with <eos>\n",
    "        for i in range(B):\n",
    "            row = seqs[i]\n",
    "            # if sequence generated <eos>, stop there\n",
    "            if (row == eos_id).any():\n",
    "                # find index of first <eos>\n",
    "                idx = (row == eos_id).nonzero(as_tuple=False)[0].item()\n",
    "                # fill everything after with <eos>\n",
    "                row[idx + 1 :] = eos_id\n",
    "        return seqs\n",
    "\n",
    "\n",
    "# build with attention\n",
    "encoder = Encoder(len(src_stoi), emb_dim, hid_dim, num_layers=layers, dropout=dropout)\n",
    "decoder = DecoderAttn(\n",
    "    len(tgt_stoi), emb_dim, hid_dim, num_layers=layers, dropout=dropout\n",
    ")\n",
    "model = Seq2SeqAttn(encoder, decoder).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472a8789",
   "metadata": {},
   "source": [
    "Bidirectional encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fcf5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40602932"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderBidirectional(nn.Module):\n",
    "  \n",
    "  def __init__(self, vocab_size, emb_dim, hid_dim, num_layers=1, dropout=0.1, model=\"gru\"):\n",
    "    \n",
    "    super().__init__()\n",
    "    self.num_layers = num_layers\n",
    "    self.hid_dim = hid_dim\n",
    "    self.model = model\n",
    "    self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "    if model.lower() == \"lstm\":\n",
    "      self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, batch_first=True, bidirectional=True,\n",
    "                       dropout=dropout if num_layers > 1 else 0.0)\n",
    "    elif model.lower() == \"gru\":\n",
    "      self.rnn = nn.GRU(emb_dim, hid_dim, num_layers=num_layers, batch_first=True, bidirectional=True,\n",
    "                       dropout=dropout if num_layers > 1 else 0.0)\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported RNN model type. Use 'lstm' or 'gru'.\")\n",
    "    \n",
    "  def forward(self, src, src_lens):\n",
    "    \n",
    "    emb = self.emb(src)\n",
    "    packed = nn.utils.rnn.pack_padded_sequence(emb, src_lens.cpu(), batch_first=True, enforce_sorted=False)\n",
    "    \n",
    "    if self.model.lower() == \"lstm\":\n",
    "        out, (h, c) = self.rnn(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        h = h.view(self.num_layers, 2, -1, self.hid_dim).sum(dim=1)\n",
    "        c = c.view(self.num_layers, 2, -1, self.hid_dim).sum(dim=1)\n",
    "        return out, (h, c)\n",
    "    elif self.model.lower() == \"gru\":\n",
    "        out, h = self.rnn(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        h = h.view(self.num_layers, 2, -1, self.hid_dim).sum(dim=1)\n",
    "        return out, h\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported RNN model type. Use 'lstm' or 'gru'.\")\n",
    "\n",
    " \n",
    "class Decoder(nn.Module):\n",
    "  \n",
    "  def __init__(self, vocab_size, emb_dim, hid_dim, num_layers=1, dropout=0.1, model=\"gru\"):\n",
    "      \n",
    "    super().__init__()\n",
    "    self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "    # decoder LSTM takes previous hidden state and current token embedding\n",
    "    if model.lower() == \"lstm\":\n",
    "      self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, batch_first=True, \n",
    "                       dropout=dropout if num_layers > 1 else 0.0)\n",
    "    elif model.lower() == \"gru\":\n",
    "      self.rnn = nn.GRU(emb_dim, hid_dim, num_layers=num_layers, batch_first=True, \n",
    "                       dropout=dropout if num_layers > 1 else 0.0)\n",
    "    else:\n",
    "      raise ValueError(\"Unsupported RNN model type. Use 'lstm' or 'gru'.\")\n",
    "    # linear layer converts hidden state to vocab logits for prediction\n",
    "    self.proj = nn.Linear(hid_dim, vocab_size)\n",
    "        \n",
    "  def forward(self, tgt_in, hidden):\n",
    "      \n",
    "    # convert target token IDs to embeddings: [B, T] -> [B, T, emb_dim]\n",
    "    emb = self.emb(tgt_in)\n",
    "    # LSTM uses previous hidden state (from encoder or prev step) to process current tokens\n",
    "    out, hidden = self.rnn(emb, hidden)\n",
    "    # project LSTM output to vocab size: [B, T, hid_dim] -> [B, T, vocab_size]\n",
    "    # return new hidden state so we can keep generating next tokens\n",
    "    return self.proj(out), hidden\n",
    "        \n",
    "class Seq2Seq(nn.Module):\n",
    "  \n",
    "    def __init__(self, enc, dec):\n",
    "      super().__init__()\n",
    "      self.encoder = enc\n",
    "      self.decoder = dec\n",
    "      \n",
    "    def forward(self, src, src_lens, tgt_in):\n",
    "      # encode source sentence, throw away encoder outputs, keep final hidden state\n",
    "      _, h = self.encoder(src, src_lens)\n",
    "      # decode using encoder's final state as initial state, tgt_in has all target tokens (teacher forcing)\n",
    "      logits, _ = self.decoder(tgt_in, h)\n",
    "      # return logits for cross-entropy loss\n",
    "      return logits\n",
    "      \n",
    "    @torch.no_grad()\n",
    "    def greedy_decode(self, src, src_lens, max_len, sos_id, eos_id):\n",
    "      \n",
    "        B = src.size(0)\n",
    "        # encode source, get initial hidden state for decoder\n",
    "        _, h = self.encoder(src, src_lens)\n",
    "        # create batch of <sos> tokens to start generation: [B, 1]\n",
    "        inputs = torch.full((B, 1), sos_id, dtype=torch.long, device=src.device)\n",
    "        outs = []\n",
    "        # generate one token at a time up to max_len\n",
    "        for _ in range(max_len):\n",
    "            # feed only last generated token with current hidden state\n",
    "            logits, h = self.decoder(inputs, h)\n",
    "            # pick token with highest probability (greedy): [B, 1]\n",
    "            nxt = logits[:, -1, :].argmax(-1, keepdim=True)\n",
    "            outs.append(nxt)\n",
    "            inputs = nxt\n",
    "            \n",
    "        # concatenate all generated tokens: [B, max_len]\n",
    "        seqs = torch.cat(outs, dim=1)\n",
    "        # truncate each sequence at first <eos> and fill rest with <eos>\n",
    "        for i in range(B):\n",
    "            row = seqs[i]\n",
    "            # if sequence generated <eos>, stop there\n",
    "            if (row == eos_id).any():\n",
    "              # find index of first <eos>\n",
    "              idx = (row == eos_id).nonzero(as_tuple=False)[0].item()\n",
    "              # fill everything after with <eos>\n",
    "              row[idx+1:] = eos_id\n",
    "        return seqs\n",
    "      \n",
    "# build encoder with source vocabulary size\n",
    "layers = 2\n",
    "encoder = EncoderBidirectional(len(src_stoi), emb_dim, hid_dim, num_layers=layers, dropout=dropout, model=\"gru\")\n",
    "# build decoder with target vocabulary size (German vocab != English vocab)\n",
    "decoder = Decoder(len(tgt_stoi), emb_dim, hid_dim, num_layers=layers, dropout=dropout, model=\"gru\")\n",
    "# wrap both in seq2seq model and move to GPU/CPU\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "# Adam optimizer for gradient descent\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# count total trainable parameters\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d54d558",
   "metadata": {},
   "source": [
    "bidirectional gru + attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3c4f909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42700084"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderBidirectionalAttn(nn.Module):\n",
    "  \n",
    "  def __init__(self, vocab_size, emb_dim, hid_dim, num_layers=1, dropout=0.1):\n",
    "    \n",
    "    super().__init__()\n",
    "    self.num_layers = num_layers\n",
    "    self.hid_dim = hid_dim\n",
    "    self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "    self.rnn = nn.GRU(emb_dim, hid_dim, num_layers=num_layers, batch_first=True, bidirectional=True,\n",
    "                       dropout=dropout if num_layers > 1 else 0.0)\n",
    "    \n",
    "  def forward(self, src, src_lens):\n",
    "    \n",
    "    emb = self.emb(src)\n",
    "    packed = nn.utils.rnn.pack_padded_sequence(emb, src_lens.cpu(), batch_first=True, enforce_sorted=False)\n",
    "    \n",
    "    out, h = self.rnn(packed)\n",
    "    out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "    h = h.view(self.num_layers, 2, -1, self.hid_dim).sum(dim=1)\n",
    "    return out, h\n",
    "\n",
    " \n",
    "class Decoder(nn.Module):\n",
    "  \n",
    "  def __init__(self, vocab_size, emb_dim, hid_dim, num_layers=1, dropout=0.1):\n",
    "      \n",
    "    super().__init__()\n",
    "    self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "    self.attention = LuongAttention(hid_dim, 2 * hid_dim, score_type='general')\n",
    "    self.rnn = nn.GRU(emb_dim + 2 * hid_dim, hid_dim, num_layers=num_layers, batch_first=True, \n",
    "                     dropout=dropout if num_layers > 1 else 0.0)\n",
    "    \n",
    "    self.proj = nn.Linear(hid_dim, vocab_size)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "  def forward_step(self, tgt_in, hidden, encoder_outputs):\n",
    "      emb = self.dropout(self.emb(tgt_in))  # [B, 1, emb_dim]\n",
    "        \n",
    "      # Extract query from hidden state\n",
    "      h = hidden[-1:].permute(1, 0, 2)\n",
    "        \n",
    "      context, attn_weights = self.attention(h, encoder_outputs)\n",
    "      emb = torch.cat([emb, context], dim=2)  # [B, 1, emb_dim + encoder_dim]\n",
    "        \n",
    "      out, hidden = self.rnn(emb, hidden)\n",
    "      logits = self.proj(out)\n",
    "      return logits, hidden, attn_weights\n",
    "        \n",
    "  def forward(self, tgt_in, hidden, encoder_outputs):\n",
    "    # Loop over sequence calling forward_step\n",
    "    batch_size, seq_len = tgt_in.size()\n",
    "    outputs = []\n",
    "        \n",
    "    for i in range(seq_len):\n",
    "      token = tgt_in[:, i:i+1]\n",
    "      logits, hidden, _ = self.forward_step(token, hidden, encoder_outputs)\n",
    "      outputs.append(logits)\n",
    "        \n",
    "    return torch.cat(outputs, dim=1), hidden\n",
    "        \n",
    "class Seq2Seq(nn.Module):\n",
    "  \n",
    "    def __init__(self, enc, dec):\n",
    "      super().__init__()\n",
    "      self.encoder = enc\n",
    "      self.decoder = dec\n",
    "      \n",
    "    def forward(self, src, src_lens, tgt_in):\n",
    "      enc_out , h = self.encoder(src, src_lens)\n",
    "      logits, _ = self.decoder(tgt_in, h, enc_out)\n",
    "      return logits\n",
    "      \n",
    "    @torch.no_grad()\n",
    "    def greedy_decode(self, src, src_lens, max_len, sos_id, eos_id):\n",
    "      \n",
    "        B = src.size(0)\n",
    "        # encode source, get initial hidden state for decoder\n",
    "        enc_out , h = self.encoder(src, src_lens)\n",
    "        # create batch of <sos> tokens to start generation: [B, 1]\n",
    "        inputs = torch.full((B, 1), sos_id, dtype=torch.long, device=src.device)\n",
    "        outs = []\n",
    "        # generate one token at a time up to max_len\n",
    "        for _ in range(max_len):\n",
    "            # feed only last generated token with current hidden state\n",
    "            logits, h, _ = self.decoder.forward_step(inputs, h, enc_out)\n",
    "            # pick token with highest probability (greedy): [B, 1]\n",
    "            nxt = logits[:, -1, :].argmax(-1, keepdim=True)\n",
    "            outs.append(nxt)\n",
    "            inputs = nxt\n",
    "            \n",
    "        # concatenate all generated tokens: [B, max_len]\n",
    "        seqs = torch.cat(outs, dim=1)\n",
    "        # truncate each sequence at first <eos> and fill rest with <eos>\n",
    "        for i in range(B):\n",
    "            row = seqs[i]\n",
    "            # if sequence generated <eos>, stop there\n",
    "            if (row == eos_id).any():\n",
    "              # find index of first <eos>\n",
    "              idx = (row == eos_id).nonzero(as_tuple=False)[0].item()\n",
    "              # fill everything after with <eos>\n",
    "              row[idx+1:] = eos_id\n",
    "        return seqs\n",
    "      \n",
    "# build encoder with source vocabulary size\n",
    "layers = 2\n",
    "encoder = EncoderBidirectionalAttn(len(src_stoi), emb_dim, hid_dim, num_layers=layers, dropout=dropout)\n",
    "# build decoder with target vocabulary size (German vocab != English vocab)\n",
    "decoder = Decoder(len(tgt_stoi), emb_dim, hid_dim, num_layers=layers, dropout=dropout)\n",
    "# wrap both in seq2seq model and move to GPU/CPU\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "# Adam optimizer for gradient descent\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# count total trainable parameters\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ca33e6",
   "metadata": {},
   "source": [
    "## 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "642b52e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = \"gru_bi_2layers_attn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8347ac02",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x512 and 1024x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m src,src_l,tgt_in,tgt_out,tgt_l \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m     15\u001b[39m     src,src_l=src.to(device), src_l.to(device); tgt_in,tgt_out=tgt_in.to(device), tgt_out.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     optimizer.zero_grad(); logits=model(src, src_l, tgt_in)\n\u001b[32m     17\u001b[39m     loss=criterion(logits.reshape(-\u001b[32m1\u001b[39m, logits.size(-\u001b[32m1\u001b[39m)), tgt_out.reshape(-\u001b[32m1\u001b[39m)); loss.backward();\n\u001b[32m     18\u001b[39m     torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m); optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/deepl_nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/deepl_nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mSeq2Seq.forward\u001b[39m\u001b[34m(self, src, src_lens, tgt_in)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, src_lens, tgt_in):\n\u001b[32m     69\u001b[39m   enc_out , h = \u001b[38;5;28mself\u001b[39m.encoder(src, src_lens)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m   logits, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/deepl_nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/deepl_nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mDecoder.forward\u001b[39m\u001b[34m(self, tgt_in, hidden, encoder_outputs)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(seq_len):\n\u001b[32m     55\u001b[39m   token = tgt_in[:, i:i+\u001b[32m1\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m   logits, hidden, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m   outputs.append(logits)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(outputs, dim=\u001b[32m1\u001b[39m), hidden\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mDecoder.forward_step\u001b[39m\u001b[34m(self, tgt_in, hidden, encoder_outputs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Extract query from hidden state\u001b[39;00m\n\u001b[32m     40\u001b[39m h = hidden[-\u001b[32m1\u001b[39m:].permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m context, attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m emb = torch.cat([emb, context], dim=\u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# [B, 1, emb_dim + encoder_dim]\u001b[39;00m\n\u001b[32m     45\u001b[39m out, hidden = \u001b[38;5;28mself\u001b[39m.rnn(emb, hidden)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/deepl_nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/deepl_nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mLuongAttention.forward\u001b[39m\u001b[34m(self, query, keys)\u001b[39m\n\u001b[32m     30\u001b[39m     scores = torch.bmm(query, keys.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m))  \u001b[38;5;66;03m# [B, 1, S]\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.score_type == \u001b[33m'\u001b[39m\u001b[33mgeneral\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     scores = torch.bmm(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mWa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m, keys.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m))  \u001b[38;5;66;03m# [B, 1, S]\u001b[39;00m\n\u001b[32m     34\u001b[39m weights = F.softmax(scores, dim=-\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# [B, 1, S]\u001b[39;00m\n\u001b[32m     35\u001b[39m context = torch.bmm(weights, keys)  \u001b[38;5;66;03m# [B, 1, H]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/deepl_nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/deepl_nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/deepl_nlp/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (64x512 and 1024x512)"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# prevent me from overwriting my file\n",
    "overwrite = True\n",
    "if os.path.exists(FILE+ \"_train.txt\"):\n",
    "  overwrite = input('y to overwrite file') == 'y'\n",
    "  print(overwrite)\n",
    "\n",
    "start_time = time.time()  # Start timing\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id_tgt, reduction='sum')\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train(); tot=0.0; toks=0\n",
    "    for src,src_l,tgt_in,tgt_out,tgt_l in train_loader:\n",
    "        src,src_l=src.to(device), src_l.to(device); tgt_in,tgt_out=tgt_in.to(device), tgt_out.to(device)\n",
    "        optimizer.zero_grad(); logits=model(src, src_l, tgt_in)\n",
    "        loss=criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1)); loss.backward();\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); optimizer.step()\n",
    "        tot+=float(loss.item()); toks+=int((tgt_out!=pad_id_tgt).sum().item())\n",
    "    tr_ppl=compute_perplexity(tot,toks); v_loss,v_toks=evaluate_nll(val_loader, model, pad_id_tgt, device); v_ppl=compute_perplexity(v_loss,v_toks)\n",
    "    print(f'Epoch {epoch:02d} | train ppl: {tr_ppl:.2f} | val ppl: {v_ppl:.2f}')\n",
    "    \n",
    "    # save ppl to file \n",
    "    if overwrite:\n",
    "      with open(FILE + \"_train.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f'Epoch {epoch:02d} | train ppl: {tr_ppl:.2f} | val ppl: {v_ppl:.2f}\\n')\n",
    "\n",
    "end_time = time.time()  # End timing\n",
    "elapsed_time = end_time - start_time\n",
    "print(f'Training time: {elapsed_time:.2f} seconds')\n",
    "if overwrite:\n",
    "  with open(FILE + \"_train.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write(f'\\nTraining time: {elapsed_time:.2f} seconds\\n')\n",
    "\n",
    "torch.save({'model_state': model.state_dict(), 'optimizer_state': optimizer.state_dict(), 'epoch': epochs, 'src_stoi': src_stoi, 'tgt_stoi': tgt_stoi, 'model_cfg': {'emb': emb_dim, 'hid': hid_dim, 'layers': layers, 'dropout': dropout}}, os.path.join(save_dir, 'checkpoint_last.pt'))\n",
    "print('Saved checkpoint:', os.path.join(save_dir, 'checkpoint_last.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23b2868",
   "metadata": {},
   "source": [
    "## 6. Evaluate: Perplexity and BLEU (Public Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fab90cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation perplexity: 4.95\n",
      "Public test perplexity: 4.90\n",
      "Public test BLEU:       31.08\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_tok = evaluate_nll(val_loader, model, pad_id_tgt, device); val_ppl = compute_perplexity(val_loss, val_tok)\n",
    "tst_loss, tst_tok = evaluate_nll(test_loader, model, pad_id_tgt, device); tst_ppl = compute_perplexity(tst_loss, tst_tok)\n",
    "bleu = evaluate_bleu(test_loader, model, tgt_itos, sos_id=sos_id, eos_id=eos_id, device=device, max_len=max_decode_len)\n",
    "print(f'Validation perplexity: {val_ppl:.2f}')\n",
    "print(f'Public test perplexity: {tst_ppl:.2f}')\n",
    "print(f'Public test BLEU:       {bleu*100:.2f}')\n",
    "\n",
    "overwrite = True\n",
    "if os.path.exists(FILE+ \"_test.txt\"):\n",
    "  overwrite = input('y to overwrite file') == 'y'\n",
    "  \n",
    "if overwrite:\n",
    "      with open(FILE + \"_test.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f'Validation perplexity: {val_ppl:.2f} | Public test perplexity: {tst_ppl:.2f} | Public test BLEU: {bleu*100:.2f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a86eea6",
   "metadata": {},
   "source": [
    "## 7. Private Test (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e60f7667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Private test split not found at dataset_splits/private_test.txt\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(private_test_path):\n",
    "    prv_pairs = read_split(private_test_path); prv_ds = TranslationDataset(prv_pairs, src_stoi, tgt_stoi)\n",
    "    prv_loader = DataLoader(prv_ds, batch_size=batch_size, shuffle=False, collate_fn=collate, num_workers=0)\n",
    "    prv_loss, prv_tok = evaluate_nll(prv_loader, model, pad_id_tgt, device); prv_ppl = compute_perplexity(prv_loss, prv_tok)\n",
    "    prv_bleu = evaluate_bleu(prv_loader, model, tgt_itos, sos_id=sos_id, eos_id=eos_id, device=device, max_len=max_decode_len)\n",
    "    print(f'Private test perplexity: {prv_ppl:.2f}')\n",
    "    print(f'Private test BLEU:       {prv_bleu*100:.2f}')\n",
    "else:\n",
    "    print('Private test split not found at', private_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91160f47",
   "metadata": {},
   "source": [
    "## 8. Export Predictions for ML‑Arena (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c084a004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "  print()\n",
    "else:\n",
    "  @torch.no_grad()\n",
    "  def decode_to_lines(loader: DataLoader, model: nn.Module, tgt_itos: List[str], sos_id: int, eos_id: int, device: torch.device, max_len: int) -> List[str]:\n",
    "      lines: List[str] = []\n",
    "      for src,src_l,tgt_in,tgt_out,tgt_l in loader:\n",
    "          src,src_l = src.to(device), src_l.to(device)\n",
    "          pred_ids = model.greedy_decode(src, src_l, max_len=max_len, sos_id=sos_id, eos_id=eos_id)\n",
    "          for b in range(src.size(0)):\n",
    "              hyp = pred_ids[b].tolist()\n",
    "              if eos_id in hyp: hyp = hyp[:hyp.index(eos_id)]\n",
    "              toks = [tgt_itos[i] for i in hyp if i != 0 and i != sos_id]\n",
    "              lines.append(' '.join(toks))\n",
    "      return lines\n",
    "  export_split = 'private'; export_format = 'tsv'; export_out = 'submissions/private_predictions.tsv'\n",
    "  os.makedirs(os.path.dirname(export_out) or '.', exist_ok=True)\n",
    "  pairs = read_split(public_test_path if export_split=='public' else private_test_path)\n",
    "  exp_ds = TranslationDataset(pairs, src_stoi, tgt_stoi); exp_loader = DataLoader(exp_ds, batch_size=batch_size, shuffle=False, collate_fn=collate, num_workers=0)\n",
    "  preds = decode_to_lines(exp_loader, model, tgt_itos, sos_id=sos_id, eos_id=eos_id, device=device, max_len=max_decode_len)\n",
    "  if export_format=='tsv':\n",
    "      with open(export_out, 'w', encoding='utf-8') as f:\n",
    "          for i,h in enumerate(preds): f.write(f'{i}\t{h}\\n')\n",
    "  elif export_format=='jsonl':\n",
    "      import json\n",
    "      with open(export_out, 'w', encoding='utf-8') as f:\n",
    "          for i,h in enumerate(preds): f.write(json.dumps({'id': i, 'hyp': h}, ensure_ascii=False)+'\\n')\n",
    "  print(f'Wrote {len(preds)} predictions to {export_out}')\n",
    "  print('Adjust if ML‑Arena requires a different schema.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
