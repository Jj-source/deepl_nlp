{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NMT Homework (Self-Contained): EN→DE\n",
        "\n",
        "Train a translation model (English→German), measure perplexity and BLEU, save a checkpoint, and optionally export predictions for ML‑Arena.\n",
        "\n",
        "Focus: experiment with architectures (LSTM w/ attention, Transformer, decoding strategies) — not boilerplate. Core evaluation functions are provided to ensure consistent scoring across students.\n",
        "\n",
        "Data: the course staff provides `dataset_splits/` in the repo root. No additional setup is needed for data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup\n",
        "Use `install.sh` or `pip install -r requirements.txt` to set up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.9.0+cu128\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "#!pip install -r requirements.txt\n",
        "import torch, sys, os, math, random\n",
        "print('PyTorch version:', torch.__version__)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "try: sys.stdout.reconfigure(line_buffering=True)\n",
        "except Exception: pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Shared Utilities (no external imports)\n",
        "Tokenization, vocabulary, dataset, collate, and fixed evaluation (PPL, NLL, BLEU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Tuple, Dict, Iterable\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "SPECIAL_TOKENS = {'pad': '<pad>', 'sos': '<sos>', 'eos': '<eos>', 'unk': '<unk>'}\n",
        "\n",
        "def simple_tokenize(s: str) -> List[str]:\n",
        "    return s.strip().lower().split()\n",
        "\n",
        "def read_split(path: str) -> List[Tuple[List[str], List[str]]]:\n",
        "    pairs: List[Tuple[List[str], List[str]]] = []\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            parts = line.rstrip('\\n').split('\t')\n",
        "            if len(parts) < 2: continue\n",
        "            pairs.append((simple_tokenize(parts[0]), simple_tokenize(parts[1])))\n",
        "    return pairs\n",
        "\n",
        "def build_vocab(seqs: Iterable[List[str]], max_size: int | None = None) -> Dict[str, int]:\n",
        "    from collections import Counter\n",
        "    c = Counter();\n",
        "    for s in seqs: c.update(s)\n",
        "    itms = c.most_common(max_size) if max_size else c.items()\n",
        "    stoi = {SPECIAL_TOKENS['pad']:0, SPECIAL_TOKENS['sos']:1, SPECIAL_TOKENS['eos']:2, SPECIAL_TOKENS['unk']:3}\n",
        "    for w,_ in itms:\n",
        "        if w not in stoi: stoi[w] = len(stoi)\n",
        "    return stoi\n",
        "\n",
        "def encode(tokens: List[str], stoi: Dict[str,int], add_sos_eos: bool=False) -> List[int]:\n",
        "    ids = [stoi.get(t, stoi[SPECIAL_TOKENS['unk']]) for t in tokens]\n",
        "    if add_sos_eos: ids = [stoi[SPECIAL_TOKENS['sos']]] + ids + [stoi[SPECIAL_TOKENS['eos']] ]\n",
        "    return ids\n",
        "\n",
        "class Example:\n",
        "    def __init__(self, s: List[int], ti: List[int], to: List[int]): self.src_ids=s; self.tgt_in_ids=ti; self.tgt_out_ids=to\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs, src_stoi, tgt_stoi):\n",
        "        self.examples: List[Example] = []\n",
        "        for src, tgt in pairs:\n",
        "            s = encode(src, src_stoi) + [src_stoi[SPECIAL_TOKENS['eos']]]\n",
        "            t = encode(tgt, tgt_stoi, add_sos_eos=True)\n",
        "            self.examples.append(Example(s, t[:-1], t[1:]))\n",
        "    def __len__(self): return len(self.examples)\n",
        "    def __getitem__(self, i): return self.examples[i]\n",
        "\n",
        "def collate_pad(batch, pad_id_src: int, pad_id_tgt: int):\n",
        "    src_max = max(len(x.src_ids) for x in batch); tgt_max = max(len(x.tgt_in_ids) for x in batch)\n",
        "    def pad_to(a, L, pad): return a + [pad]*(L-len(a))\n",
        "    src    = torch.tensor([pad_to(x.src_ids,    src_max, pad_id_src) for x in batch])\n",
        "    tgt_in = torch.tensor([pad_to(x.tgt_in_ids, tgt_max, pad_id_tgt) for x in batch])\n",
        "    tgt_out= torch.tensor([pad_to(x.tgt_out_ids,tgt_max, pad_id_tgt) for x in batch])\n",
        "    src_l  = torch.tensor([len(x.src_ids)    for x in batch])\n",
        "    tgt_l  = torch.tensor([len(x.tgt_out_ids)for x in batch])\n",
        "    return src, src_l, tgt_in, tgt_out, tgt_l\n",
        "\n",
        "def compute_perplexity(loss_sum: float, token_count: int) -> float:\n",
        "    if token_count==0: return float('inf')\n",
        "    try: return float(math.exp(loss_sum/token_count))\n",
        "    except OverflowError: return float('inf')\n",
        "\n",
        "def corpus_bleu(refs: List[List[str]], hyps: List[List[str]], max_order: int=4, smooth: bool=True) -> float:\n",
        "    from collections import Counter\n",
        "    def ngrams(t,n): return Counter([tuple(t[i:i+n]) for i in range(len(t)-n+1)])\n",
        "    m=[0]*max_order; p=[0]*max_order; rl=0; hl=0\n",
        "    for r,h in zip(refs,hyps):\n",
        "        rl+=len(r); hl+=len(h);\n",
        "        for n in range(1,max_order+1):\n",
        "            R=ngrams(r,n); H=ngrams(h,n);\n",
        "            m[n-1]+=sum(min(c,H[g]) for g,c in R.items()); p[n-1]+=max(len(h)-n+1,0)\n",
        "    prec=[(m[i]+1)/(p[i]+1) if smooth else (m[i]/p[i] if p[i]>0 else 0.0) for i in range(max_order)]\n",
        "    geo=math.exp(sum((1/max_order)*math.log(x) for x in prec if x>0)) if min(prec)>0 else 0.0\n",
        "    bp=1.0 if hl>rl else math.exp(1-rl/max(1,hl))\n",
        "    return float(geo*bp)\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_nll(loader: DataLoader, model: nn.Module, pad_id_tgt: int, device: torch.device):\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=pad_id_tgt, reduction='sum')\n",
        "    model.eval(); tot=0.0; toks=0\n",
        "    for src,src_l,tgt_in,tgt_out,tgt_l in loader:\n",
        "        src,src_l = src.to(device), src_l.to(device)\n",
        "        tgt_in,tgt_out = tgt_in.to(device), tgt_out.to(device)\n",
        "        logits = model(src, src_l, tgt_in)\n",
        "        loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
        "        tot+=float(loss.item()); toks+=int((tgt_out!=pad_id_tgt).sum().item())\n",
        "    return tot, toks\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_bleu(loader: DataLoader, model: nn.Module, tgt_itos: List[str], sos_id: int, eos_id: int, device: torch.device, max_len: int=100):\n",
        "    model.eval(); refs=[]; hyps=[]\n",
        "    for src,src_l,tgt_in,tgt_out,tgt_l in loader:\n",
        "        src,src_l = src.to(device), src_l.to(device)\n",
        "        pred = model.greedy_decode(src, src_l, max_len=max_len, sos_id=sos_id, eos_id=eos_id)\n",
        "        for b in range(src.size(0)):\n",
        "            ref_ids = tgt_out[b].tolist(); hyp_ids = pred[b].tolist()\n",
        "            if eos_id in ref_ids: ref_ids = ref_ids[:ref_ids.index(eos_id)]\n",
        "            if eos_id in hyp_ids: hyp_ids = hyp_ids[:hyp_ids.index(eos_id)]\n",
        "            refs.append([tgt_itos[i] for i in ref_ids if i!=0])\n",
        "            hyps.append([tgt_itos[i] for i in hyp_ids if i!=0 and i!=sos_id])\n",
        "    return float(corpus_bleu(refs, hyps))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Paths and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public test path: dataset_splits/public_test.txt\n"
          ]
        }
      ],
      "source": [
        "set_seed(42)\n",
        "train_path = 'dataset_splits/train.txt'\n",
        "val_path   = 'dataset_splits/val.txt'\n",
        "public_test_path = 'dataset_splits/public_test.txt'\n",
        "if not os.path.exists(public_test_path):\n",
        "    alt = 'dataset_splits/test_public.txt'\n",
        "    public_test_path = alt if os.path.exists(alt) else public_test_path\n",
        "private_test_path = 'dataset_splits/private_test.txt'\n",
        "src_vocab_size = 30000; tgt_vocab_size = 30000\n",
        "emb_dim = 256; hid_dim = 512; layers = 1; dropout = 0.1\n",
        "batch_size = 64; epochs = 5; lr = 3e-4; max_decode_len = 100\n",
        "save_dir = 'checkpoints'; os.makedirs(save_dir, exist_ok=True)\n",
        "print('Public test path:', public_test_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Data and Build Vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading splits...\n",
            "Train: 226,997 | Val: 32,428 | Public test: 32,428\n",
            "Vocab sizes — src: 30004 tgt: 30004\n"
          ]
        }
      ],
      "source": [
        "print('Loading splits...')\n",
        "train_pairs = read_split(train_path); val_pairs = read_split(val_path); test_pairs = read_split(public_test_path)\n",
        "print(f'Train: {len(train_pairs):,} | Val: {len(val_pairs):,} | Public test: {len(test_pairs):,}')\n",
        "src_stoi = build_vocab((s for s,_ in train_pairs), max_size=src_vocab_size)\n",
        "tgt_stoi = build_vocab((t for _,t in train_pairs), max_size=tgt_vocab_size)\n",
        "pad_id_src = src_stoi[SPECIAL_TOKENS['pad']]; pad_id_tgt = tgt_stoi[SPECIAL_TOKENS['pad']]\n",
        "sos_id = tgt_stoi[SPECIAL_TOKENS['sos']]; eos_id = tgt_stoi[SPECIAL_TOKENS['eos']]\n",
        "train_ds = TranslationDataset(train_pairs, src_stoi, tgt_stoi); val_ds = TranslationDataset(val_pairs, src_stoi, tgt_stoi); test_ds = TranslationDataset(test_pairs, src_stoi, tgt_stoi)\n",
        "collate = lambda b: collate_pad(b, pad_id_src, pad_id_tgt)\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  collate_fn=collate, num_workers=0)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, collate_fn=collate, num_workers=0)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, collate_fn=collate, num_workers=0)\n",
        "tgt_itos = [None]*len(tgt_stoi);\n",
        "for w,i in tgt_stoi.items():\n",
        "    if 0<=i<len(tgt_itos): tgt_itos[i]=w\n",
        "print('Vocab sizes — src:', len(src_stoi), 'tgt:', len(tgt_stoi))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Build Model (Your Playground)\n",
        "Keep the forward/greedy_decode contract so evaluation works. Try adding attention, GRU, Transformer, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "33908020"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class Encoder(nn.Module):\n",
        "  \n",
        "  def __init__(self, vocab_size, emb_dim, hid_dim, num_layers=1, dropout=0.1):\n",
        "    \n",
        "    super().__init__()\n",
        "    # converts token IDs to dense vectors, padding_idx=0 means <pad> gets zeroed out and doesn't learn\n",
        "    self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "    # LSTM takes embeddings and produces hidden states, batch_first means input is [batch, seq, features]\n",
        "    # dropout between LSTM layers only makes sense if num_layers > 1, otherwise skip it\n",
        "    self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, batch_first=True, \n",
        "                       dropout=dropout if num_layers > 1 else 0.0)\n",
        "    \n",
        "  def forward(self, src, src_lens):\n",
        "    \n",
        "    # turn token IDs into vectors: [B, S] -> [B, S, emb_dim]\n",
        "    emb = self.emb(src)\n",
        "    # pack removes padding so LSTM doesn't waste computation on <pad> tokens\n",
        "    # enforce_sorted=False handles sequences in any order (slightly slower but easier)\n",
        "    packed = nn.utils.rnn.pack_padded_sequence(emb, src_lens.cpu(), batch_first=True, enforce_sorted=False)\n",
        "    # LSTM returns: outputs for all timesteps + final hidden state (h, c)\n",
        "    out, (h, c) = self.rnn(packed)\n",
        "    # unpack puts padding back so we get regular [B, S, hid_dim] tensor\n",
        "    out, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
        "    # return both: encoder outputs and final (h, c) which initializes decoder\n",
        "    return out, (h, c)\n",
        "  \n",
        "class Decoder(nn.Module):\n",
        "  \n",
        "  def __init__(self, vocab_size, emb_dim, hid_dim, num_layers=1, dropout=0.1):\n",
        "      \n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "    # decoder LSTM takes previous hidden state and current token embedding\n",
        "    self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, batch_first=True, \n",
        "                       dropout=dropout if num_layers > 1 else 0.0)\n",
        "    # linear layer converts hidden state to vocab logits for prediction\n",
        "    self.proj = nn.Linear(hid_dim, vocab_size)\n",
        "        \n",
        "  def forward(self, tgt_in, hidden):\n",
        "      \n",
        "    # convert target token IDs to embeddings: [B, T] -> [B, T, emb_dim]\n",
        "    emb = self.emb(tgt_in)\n",
        "    # LSTM uses previous hidden state (from encoder or prev step) to process current tokens\n",
        "    out, hidden = self.rnn(emb, hidden)\n",
        "    # project LSTM output to vocab size: [B, T, hid_dim] -> [B, T, vocab_size]\n",
        "    # return new hidden state so we can keep generating next tokens\n",
        "    return self.proj(out), hidden\n",
        "        \n",
        "class Seq2Seq(nn.Module):\n",
        "  \n",
        "    def __init__(self, enc, dec):\n",
        "      super().__init__()\n",
        "      self.encoder = enc\n",
        "      self.decoder = dec\n",
        "      \n",
        "    def forward(self, src, src_lens, tgt_in):\n",
        "      # encode source sentence, throw away encoder outputs, keep final hidden state\n",
        "      _, h = self.encoder(src, src_lens)\n",
        "      # decode using encoder's final state as initial state, tgt_in has all target tokens (teacher forcing)\n",
        "      logits, _ = self.decoder(tgt_in, h)\n",
        "      # return logits for cross-entropy loss\n",
        "      return logits\n",
        "      \n",
        "    @torch.no_grad()\n",
        "    def greedy_decode(self, src, src_lens, max_len, sos_id, eos_id):\n",
        "      \n",
        "        B = src.size(0)\n",
        "        # encode source, get initial hidden state for decoder\n",
        "        _, h = self.encoder(src, src_lens)\n",
        "        # create batch of <sos> tokens to start generation: [B, 1]\n",
        "        inputs = torch.full((B, 1), sos_id, dtype=torch.long, device=src.device)\n",
        "        outs = []\n",
        "        # generate one token at a time up to max_len\n",
        "        for _ in range(max_len):\n",
        "            # feed only last generated token with current hidden state\n",
        "            logits, h = self.decoder(inputs[:, -1:].contiguous(), h)\n",
        "            # pick token with highest probability (greedy): [B, 1]\n",
        "            nxt = logits[:, -1, :].argmax(-1, keepdim=True)\n",
        "            outs.append(nxt)\n",
        "            # append new token to sequence for next iteration (not actually needed but kept for consistency)\n",
        "            inputs = torch.cat([inputs, nxt], dim=1)\n",
        "            \n",
        "        # concatenate all generated tokens: [B, max_len]\n",
        "        seqs = torch.cat(outs, dim=1)\n",
        "        # truncate each sequence at first <eos> and fill rest with <eos>\n",
        "        for i in range(B):\n",
        "            row = seqs[i]\n",
        "            # if sequence generated <eos>, stop there\n",
        "            if (row == eos_id).any():\n",
        "              # find index of first <eos>\n",
        "              idx = (row == eos_id).nonzero(as_tuple=False)[0].item()\n",
        "              # fill everything after with <eos>\n",
        "              row[idx+1:] = eos_id\n",
        "        return seqs\n",
        "      \n",
        "# build encoder with source vocabulary size\n",
        "encoder = Encoder(len(src_stoi), emb_dim, hid_dim, num_layers=layers, dropout=dropout)\n",
        "# build decoder with target vocabulary size (German vocab != English vocab)\n",
        "decoder = Decoder(len(tgt_stoi), emb_dim, hid_dim, num_layers=layers, dropout=dropout)\n",
        "# wrap both in seq2seq model and move to GPU/CPU\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "# Adam optimizer for gradient descent\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "# count total trainable parameters\n",
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2e8ee83a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# attentions\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c30212ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class Decoder(nn.Module):\n",
        "  \n",
        "  def __init__(self, vocab_size, emb_dim, hid_dim, num_layers=1, dropout=0.1):\n",
        "      \n",
        "    super().__init__()\n",
        "    self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "    # decoder LSTM takes previous hidden state and current token embedding\n",
        "    self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=num_layers, batch_first=True, \n",
        "                       dropout=dropout if num_layers > 1 else 0.0)\n",
        "    # linear layer converts hidden state to vocab logits for prediction\n",
        "    self.proj = nn.Linear(hid_dim, vocab_size)\n",
        "        \n",
        "  def forward(self, tgt_in, hidden):\n",
        "      \n",
        "    # convert target token IDs to embeddings: [B, T] -> [B, T, emb_dim]\n",
        "    emb = self.emb(tgt_in)\n",
        "    # LSTM uses previous hidden state (from encoder or prev step) to process current tokens\n",
        "    out, hidden = self.rnn(emb, hidden)\n",
        "    # project LSTM output to vocab size: [B, T, hid_dim] -> [B, T, vocab_size]\n",
        "    # return new hidden state so we can keep generating next tokens\n",
        "    return self.proj(out), hidden\n",
        "        \n",
        "class Seq2Seq(nn.Module):\n",
        "  \n",
        "    def __init__(self, enc, dec):\n",
        "      super().__init__()\n",
        "      self.encoder = enc\n",
        "      self.decoder = dec\n",
        "      \n",
        "    def forward(self, src, src_lens, tgt_in):\n",
        "      # encode source sentence, throw away encoder outputs, keep final hidden state\n",
        "      _, h = self.encoder(src, src_lens)\n",
        "      # decode using encoder's final state as initial state, tgt_in has all target tokens (teacher forcing)\n",
        "      logits, _ = self.decoder(tgt_in, h)\n",
        "      # return logits for cross-entropy loss\n",
        "      return logits\n",
        "      \n",
        "    @torch.no_grad()\n",
        "    def greedy_decode(self, src, src_lens, max_len, sos_id, eos_id):\n",
        "      \n",
        "        B = src.size(0)\n",
        "        # encode source, get initial hidden state for decoder\n",
        "        _, h = self.encoder(src, src_lens)\n",
        "        # create batch of <sos> tokens to start generation: [B, 1]\n",
        "        inputs = torch.full((B, 1), sos_id, dtype=torch.long, device=src.device)\n",
        "        outs = []\n",
        "        # generate one token at a time up to max_len\n",
        "        for _ in range(max_len):\n",
        "            # feed only last generated token with current hidden state\n",
        "            logits, h = self.decoder(inputs[:, -1:].contiguous(), h)\n",
        "            # pick token with highest probability (greedy): [B, 1]\n",
        "            nxt = logits[:, -1, :].argmax(-1, keepdim=True)\n",
        "            outs.append(nxt)\n",
        "            # append new token to sequence for next iteration (not actually needed but kept for consistency)\n",
        "            inputs = torch.cat([inputs, nxt], dim=1)\n",
        "            \n",
        "        # concatenate all generated tokens: [B, max_len]\n",
        "        seqs = torch.cat(outs, dim=1)\n",
        "        # truncate each sequence at first <eos> and fill rest with <eos>\n",
        "        for i in range(B):\n",
        "            row = seqs[i]\n",
        "            # if sequence generated <eos>, stop there\n",
        "            if (row == eos_id).any():\n",
        "              # find index of first <eos>\n",
        "              idx = (row == eos_id).nonzero(as_tuple=False)[0].item()\n",
        "              # fill everything after with <eos>\n",
        "              row[idx+1:] = eos_id\n",
        "        return seqs\n",
        "      \n",
        "# build encoder with source vocabulary size\n",
        "encoder = Encoder(len(src_stoi), emb_dim, hid_dim, num_layers=layers, dropout=dropout)\n",
        "# build decoder with target vocabulary size (German vocab != English vocab)\n",
        "decoder = Decoder(len(tgt_stoi), emb_dim, hid_dim, num_layers=layers, dropout=dropout)\n",
        "# wrap both in seq2seq model and move to GPU/CPU\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "# Adam optimizer for gradient descent\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "# count total trainable parameters\n",
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "642b52e4",
      "metadata": {},
      "outputs": [],
      "source": [
        "FILE = \"lstm_baseline\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 | train ppl: 66.19 | val ppl: 22.41\n",
            "Epoch 02 | train ppl: 14.79 | val ppl: 11.49\n",
            "Epoch 03 | train ppl: 7.80 | val ppl: 8.30\n",
            "Epoch 04 | train ppl: 5.17 | val ppl: 6.88\n",
            "Epoch 05 | train ppl: 3.86 | val ppl: 6.16\n",
            "Saved checkpoint: checkpoints/checkpoint_last.pt\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# prevent me from overwriting my file\n",
        "overwrite = True\n",
        "if os.path.exists(FILE+ \"_train.txt\"):\n",
        "  overwrite = input('y to overwrite file') == 'y'\n",
        "  print(overwrite)\n",
        "\n",
        "start_time = time.time()  # Start timing\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_id_tgt, reduction='sum')\n",
        "for epoch in range(1, epochs+1):\n",
        "    model.train(); tot=0.0; toks=0\n",
        "    for src,src_l,tgt_in,tgt_out,tgt_l in train_loader:\n",
        "        src,src_l=src.to(device), src_l.to(device); tgt_in,tgt_out=tgt_in.to(device), tgt_out.to(device)\n",
        "        optimizer.zero_grad(); logits=model(src, src_l, tgt_in)\n",
        "        loss=criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1)); loss.backward();\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); optimizer.step()\n",
        "        tot+=float(loss.item()); toks+=int((tgt_out!=pad_id_tgt).sum().item())\n",
        "    tr_ppl=compute_perplexity(tot,toks); v_loss,v_toks=evaluate_nll(val_loader, model, pad_id_tgt, device); v_ppl=compute_perplexity(v_loss,v_toks)\n",
        "    print(f'Epoch {epoch:02d} | train ppl: {tr_ppl:.2f} | val ppl: {v_ppl:.2f}')\n",
        "    \n",
        "    # save ppl to file \n",
        "    if overwrite:\n",
        "      with open(FILE + \"_train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f'Epoch {epoch:02d} | train ppl: {tr_ppl:.2f} | val ppl: {v_ppl:.2f}\\n')\n",
        "\n",
        "end_time = time.time()  # End timing\n",
        "elapsed_time = end_time - start_time\n",
        "print(f'Training time: {elapsed_time:.2f} seconds')\n",
        "if overwrite:\n",
        "  with open(FILE + \"_train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(f'\\nTraining time: {elapsed_time:.2f} seconds\\n')\n",
        "\n",
        "torch.save({'model_state': model.state_dict(), 'optimizer_state': optimizer.state_dict(), 'epoch': epochs, 'src_stoi': src_stoi, 'tgt_stoi': tgt_stoi, 'model_cfg': {'emb': emb_dim, 'hid': hid_dim, 'layers': layers, 'dropout': dropout}}, os.path.join(save_dir, 'checkpoint_last.pt'))\n",
        "print('Saved checkpoint:', os.path.join(save_dir, 'checkpoint_last.pt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluate: Perplexity and BLEU (Public Test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation perplexity: 6.16\n",
            "Public test perplexity: 6.11\n",
            "Public test BLEU:       25.40\n"
          ]
        }
      ],
      "source": [
        "val_loss, val_tok = evaluate_nll(val_loader, model, pad_id_tgt, device); val_ppl = compute_perplexity(val_loss, val_tok)\n",
        "tst_loss, tst_tok = evaluate_nll(test_loader, model, pad_id_tgt, device); tst_ppl = compute_perplexity(tst_loss, tst_tok)\n",
        "bleu = evaluate_bleu(test_loader, model, tgt_itos, sos_id=sos_id, eos_id=eos_id, device=device, max_len=max_decode_len)\n",
        "print(f'Validation perplexity: {val_ppl:.2f}')\n",
        "print(f'Public test perplexity: {tst_ppl:.2f}')\n",
        "print(f'Public test BLEU:       {bleu*100:.2f}')\n",
        "\n",
        "overwrite = True\n",
        "if os.path.exists(FILE+ \"_test.txt\"):\n",
        "  overwrite = input('y to overwrite file') == 'y'\n",
        "  \n",
        "if overwrite:\n",
        "      with open(FILE + \"_test.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(f'Validation perplexity: {val_ppl:.2f} | Public test perplexity: {tst_ppl:.2f} | Public test BLEU: {bleu*100:.2f}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Private Test (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Private test split not found at dataset_splits/private_test.txt\n"
          ]
        }
      ],
      "source": [
        "if os.path.exists(private_test_path):\n",
        "    prv_pairs = read_split(private_test_path); prv_ds = TranslationDataset(prv_pairs, src_stoi, tgt_stoi)\n",
        "    prv_loader = DataLoader(prv_ds, batch_size=batch_size, shuffle=False, collate_fn=collate, num_workers=0)\n",
        "    prv_loss, prv_tok = evaluate_nll(prv_loader, model, pad_id_tgt, device); prv_ppl = compute_perplexity(prv_loss, prv_tok)\n",
        "    prv_bleu = evaluate_bleu(prv_loader, model, tgt_itos, sos_id=sos_id, eos_id=eos_id, device=device, max_len=max_decode_len)\n",
        "    print(f'Private test perplexity: {prv_ppl:.2f}')\n",
        "    print(f'Private test BLEU:       {prv_bleu*100:.2f}')\n",
        "else:\n",
        "    print('Private test split not found at', private_test_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Export Predictions for ML‑Arena (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote 32428 predictions to submissions/private_predictions.tsv\n",
            "Adjust if ML‑Arena requires a different schema.\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def decode_to_lines(loader: DataLoader, model: nn.Module, tgt_itos: List[str], sos_id: int, eos_id: int, device: torch.device, max_len: int) -> List[str]:\n",
        "    lines: List[str] = []\n",
        "    for src,src_l,tgt_in,tgt_out,tgt_l in loader:\n",
        "        src,src_l = src.to(device), src_l.to(device)\n",
        "        pred_ids = model.greedy_decode(src, src_l, max_len=max_len, sos_id=sos_id, eos_id=eos_id)\n",
        "        for b in range(src.size(0)):\n",
        "            hyp = pred_ids[b].tolist()\n",
        "            if eos_id in hyp: hyp = hyp[:hyp.index(eos_id)]\n",
        "            toks = [tgt_itos[i] for i in hyp if i != 0 and i != sos_id]\n",
        "            lines.append(' '.join(toks))\n",
        "    return lines\n",
        "export_split = 'private'; export_format = 'tsv'; export_out = 'submissions/private_predictions.tsv'\n",
        "os.makedirs(os.path.dirname(export_out) or '.', exist_ok=True)\n",
        "pairs = read_split(public_test_path if export_split=='public' else private_test_path)\n",
        "exp_ds = TranslationDataset(pairs, src_stoi, tgt_stoi); exp_loader = DataLoader(exp_ds, batch_size=batch_size, shuffle=False, collate_fn=collate, num_workers=0)\n",
        "preds = decode_to_lines(exp_loader, model, tgt_itos, sos_id=sos_id, eos_id=eos_id, device=device, max_len=max_decode_len)\n",
        "if export_format=='tsv':\n",
        "    with open(export_out, 'w', encoding='utf-8') as f:\n",
        "        for i,h in enumerate(preds): f.write(f'{i}\t{h}\\n')\n",
        "elif export_format=='jsonl':\n",
        "    import json\n",
        "    with open(export_out, 'w', encoding='utf-8') as f:\n",
        "        for i,h in enumerate(preds): f.write(json.dumps({'id': i, 'hyp': h}, ensure_ascii=False)+'\\n')\n",
        "print(f'Wrote {len(preds)} predictions to {export_out}')\n",
        "print('Adjust if ML‑Arena requires a different schema.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
