{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tiny Shakespeare RNN Walkthrough\n",
        "\n",
        "This notebook explains, step by step, how the handcrafted PyTorch RNN from `11_rnn_pytorch_custom_ts.py` works. It is written for beginners who are comfortable with basic Python and want to understand how recurrent language models operate on real text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Prerequisites\n",
        "\n",
        "We use the Tiny Shakespeare corpus stored in `tiny.txt`. The companion script `08_prepare_tiny_shakespeare.py` tokenizes the dataset and saves ready-to-use NumPy arrays in the `processed/` folder.\n",
        "\n",
        "Run the following code cell once to make sure those processed files exist (it will call the preparation script automatically if needed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import subprocess\n",
        "\n",
        "project_dir = Path.cwd()\n",
        "processed_dir = project_dir / 'processed'\n",
        "if not processed_dir.exists():\n",
        "    print('Processed data not found. Running 08_prepare_tiny_shakespeare.py ...')\n",
        "    subprocess.run(['python', '08_prepare_tiny_shakespeare.py'], check=True)\n",
        "else:\n",
        "    print('Found processed data directory:', processed_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Imports and Device Setup\n",
        "\n",
        "We stick to standard PyTorch and NumPy. Setting the random seed helps reproducibility, so the generated text should look similar each time you run the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(5)\n",
        "np.random.seed(5)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load the Processed Character Data\n",
        "\n",
        "The preparation script saved two files that we need:\n",
        "* `tiny_shakespeare_chars.json`: metadata such as the character vocabulary and sequence length.\n",
        "* `tiny_shakespeare_char_windows.npy`: overlapping windows of characters for training.\n",
        "\n",
        "We load them and inspect their shapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = project_dir / 'processed'\n",
        "char_json_path = data_dir / 'tiny_shakespeare_chars.json'\n",
        "char_windows_path = data_dir / 'tiny_shakespeare_char_windows.npy'\n",
        "\n",
        "char_data = json.loads(char_json_path.read_text(encoding='utf-8'))\n",
        "char_windows = np.load(char_windows_path)\n",
        "\n",
        "chars = char_data['chars']\n",
        "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
        "idx_to_char = {idx: ch for ch, idx in char_to_idx.items()}\n",
        "sequence_length = char_data['sequence_length']\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print('Character vocabulary size:', vocab_size)\n",
        "print('Sequence window length:', sequence_length)\n",
        "print('Number of windows in file:', len(char_windows))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset contains hundreds of thousands of windows. Training on all of them would take a while, so we randomly select a manageable subset that still captures the flavour of Shakespeare's writing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(1)\n",
        "subset_size = min(12000, len(char_windows))\n",
        "indices = rng.choice(len(char_windows), size=subset_size, replace=False)\n",
        "train_windows = char_windows[indices]\n",
        "print('Using', len(train_windows), 'windows for training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Understanding the Model Architecture\n",
        "\n",
        "We build a custom RNN cell using PyTorch's `nn.Module`. At each time step it:\n",
        "1. Receives the current character as a one-hot vector.\n",
        "2. Updates the hidden state with a tanh activation.\n",
        "3. Produces logits (raw scores) for the next character.\n",
        "\n",
        "Weights are initialized manually to keep the implementation close to the NumPy version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomCharRNN(nn.Module):\n",
        "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.Wxh = nn.Parameter(torch.randn(hidden_size, input_size) * 0.05)\n",
        "        self.Whh = nn.Parameter(torch.randn(hidden_size, hidden_size) * 0.05)\n",
        "        self.bh = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.Why = nn.Parameter(torch.randn(output_size, hidden_size) * 0.05)\n",
        "        self.by = nn.Parameter(torch.zeros(output_size))\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor, h0: torch.Tensor | None = None):\n",
        "        if h0 is None:\n",
        "            h_t = inputs.new_zeros(self.hidden_size)\n",
        "        else:\n",
        "            h_t = h0\n",
        "\n",
        "        outputs = []\n",
        "        for x_t in inputs:\n",
        "            h_t = torch.tanh(self.Wxh @ x_t + self.Whh @ h_t + self.bh)\n",
        "            y_t = self.Why @ h_t + self.by\n",
        "            outputs.append(y_t)\n",
        "        return torch.stack(outputs), h_t\n",
        "\n",
        "model = CustomCharRNN(vocab_size, hidden_size=192, output_size=vocab_size).to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Setup\n",
        "\n",
        "To keep the notebook lightweight we train for only a handful of epochs with the Adam optimizer. Each training window contributes `sequence_length` steps, so even a few epochs cover many character transitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = 10\n",
        "print('Training for', epochs, 'epochs on device', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper: Convert a window to tensors\n",
        "\n",
        "Each window stores indices. We convert them to one-hot vectors for inputs and plain integer targets for the next character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def window_to_tensors(window: np.ndarray):\n",
        "    inputs_idx = torch.tensor(window[:-1], dtype=torch.long, device=device)\n",
        "    targets = torch.tensor(window[1:], dtype=torch.long, device=device)\n",
        "    inputs = F.one_hot(inputs_idx, num_classes=vocab_size).float()\n",
        "    return inputs, targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Loop\n",
        "\n",
        "We track the average loss per epoch. Lower loss means the model is getting better at predicting the next character, although simple RNNs still struggle with long-range structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_history = []\n",
        "for epoch in range(1, epochs + 1):\n",
        "    total_loss = 0.0\n",
        "    np.random.shuffle(train_windows)\n",
        "    for window in train_windows:\n",
        "        inputs, targets = window_to_tensors(window)\n",
        "        logits, _ = model(inputs)\n",
        "        loss = criterion(logits, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_windows)\n",
        "    loss_history.append(avg_loss)\n",
        "    print(f'Epoch {epoch:02d} - average loss: {avg_loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Plot Training Loss\n",
        "\n",
        "A simple plot helps us see the training trend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(loss_history, marker='o')\n",
        "plt.title('Average Training Loss per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Generating New Text\n",
        "\n",
        "We sample characters one at a time. The `temperature` parameter controls randomness: lower values make the model pick high-probability characters, while higher values encourage diversity (at the risk of gibberish)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample(model, seed: str, length: int = 400, temperature: float = 0.8):\n",
        "    model.eval()\n",
        "    generated = list(seed)\n",
        "    with torch.no_grad():\n",
        "        h_t = None\n",
        "        for ch in seed[:-1]:\n",
        "            vec = F.one_hot(torch.tensor(char_to_idx[ch]).to(device), num_classes=vocab_size).float()\n",
        "            _, h_t = model(vec.unsqueeze(0), h_t)\n",
        "\n",
        "        char = seed[-1]\n",
        "        for _ in range(length):\n",
        "            idx = char_to_idx.get(char, 0)\n",
        "            vec = F.one_hot(torch.tensor(idx).to(device), num_classes=vocab_size).float()\n",
        "            logits, h_t = model(vec.unsqueeze(0), h_t)\n",
        "            logits = logits[-1] / temperature\n",
        "            probs = F.softmax(logits, dim=0)\n",
        "            idx = torch.multinomial(probs, num_samples=1).item()\n",
        "            char = idx_to_char[idx]\n",
        "            generated.append(char)\n",
        "    model.train()\n",
        "    return ''.join(generated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_text = 'ROMEO:\n'\n",
        "generated = sample(model, seed_text, length=400, temperature=0.8)\n",
        "print(generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try experimenting with different seeds and temperature values (e.g., `0.5`, `1.2`) to hear how the model changes its tone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Key Takeaways\n",
        "\n",
        "* **Handcrafted weights:** We built an RNN cell manually, which demystifies what `nn.RNN` does under the hood.\n",
        "* **Autograd convenience:** Even though the cell is custom, PyTorch handled the gradient calculations.\n",
        "* **Ungated limitations:** Vanilla RNNs forget long-range patterns and often repeat words. LSTM or GRU cells add gating to address this.\n",
        "* **Data hunger:** More data and longer training improve realism, but also increase compute time.\n",
        "\n",
        "Next steps you can try:\n",
        "1. Replace the hidden cell with an LSTM-like implementation.\n",
        "2. Batch the training windows for faster training.\n",
        "3. Switch to a word-level model by adapting the preprocessing artifacts."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}